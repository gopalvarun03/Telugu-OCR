{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from cnn import *\n",
    "from Decoders import *\n",
    "from utils import *\n",
    "\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bases:  108\n",
      "Vms:  19\n",
      "Cms:  36\n"
     ]
    }
   ],
   "source": [
    "acchulu = ['అ', 'ఆ', 'ఇ', 'ఈ', 'ఉ', 'ఊ', 'ఋ', 'ౠ', 'ఌ', 'ౡ', 'ఎ', 'ఏ', 'ఐ', 'ఒ', 'ఓ', 'ఔ', 'అం', 'అః']\n",
    "hallulu = ['క', 'ఖ', 'గ', 'ఘ', 'ఙ',\n",
    "           'చ', 'ఛ', 'జ', 'ఝ', 'ఞ',\n",
    "           'ట', 'ఠ', 'డ', 'ఢ', 'ణ',\n",
    "           'త', 'థ', 'ద', 'ధ', 'న',\n",
    "           'ప', 'ఫ', 'బ', 'భ', 'మ',\n",
    "           'య', 'ర', 'ల', 'వ', 'శ', 'ష', 'స', 'హ', 'ళ', 'క్ష', 'ఱ']\n",
    "vallulu = ['ా', 'ి', 'ీ', 'ు' , 'ూ', 'ృ', 'ౄ', 'ె', 'ే', 'ై', 'ొ', 'ో', 'ౌ', 'ం', 'ః', 'ఁ', 'ౕ', 'ౖ', 'ౢ' ]\n",
    "connector = ['్']\n",
    "numbers = ['౦', '౧', '౨', '౩', '౪', '౫', '౬', '౭', '౮', '౯']\n",
    "splcharacters= [' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')',\n",
    "              '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[',\n",
    "              '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '1','2', '3', '4', '5', '6', '7', '8', '9', '0', 'ఽ']\n",
    "spl = splcharacters + numbers\n",
    "\n",
    "bases = acchulu + hallulu + spl\n",
    "vms = vallulu\n",
    "cms = hallulu\n",
    "\n",
    "print(\"Bases: \", len(bases))\n",
    "print(\"Vms: \", len(vms))\n",
    "print(\"Cms: \", len(cms))\n",
    "\n",
    "characters = bases+vms+cms+connector\n",
    "\n",
    "base_mapping = {}\n",
    "i = 2\n",
    "for x in bases:\n",
    "  base_mapping[x] = i\n",
    "  i+=1\n",
    "\n",
    "vm_mapping = {}\n",
    "i = 2\n",
    "for x in vms:\n",
    "  vm_mapping[x] = i\n",
    "  i+=1\n",
    "\n",
    "cm_mapping = {}\n",
    "i = 2\n",
    "for x in cms:\n",
    "  cm_mapping[x] = i\n",
    "  i+=1\n",
    "\n",
    "  \n",
    "# creates a list of ductionaries with each dictionary reporesenting a term\n",
    "def wordsDicts(s):\n",
    "  List = []\n",
    "  for i in range(len(s)):\n",
    "    x = s[i]\n",
    "    prev = ''\n",
    "    if i > 0: prev = s[i-1]\n",
    "    #----------------------------------is it a base term-----------------------\n",
    "    if((x in acchulu or x in hallulu)  and prev != connector[0]):\n",
    "      List.append({})\n",
    "      List[-1]['base'] = x\n",
    "    #----------------------------if it is a consonant modifier-----------------\n",
    "    elif x in hallulu and prev == connector[0]:\n",
    "      if(len(List) == 0):\n",
    "        print(x)\n",
    "      if('cm' not in List[-1]): List[-1]['cm'] = []\n",
    "      List[len(List)-1]['cm'].append(x)\n",
    "\n",
    "      #---------------------------if it is a vowel modifier--------------------\n",
    "    elif x in vallulu:\n",
    "      if(len(List) == 0):\n",
    "        print(x)\n",
    "\n",
    "      if('vm' not in List[-1]): List[-1]['vm'] = []\n",
    "      List[len(List)-1]['vm'].append(x)\n",
    "\n",
    "      #----------------------------it is a spl character-----------------------\n",
    "    elif x in spl:\n",
    "      List.append({})\n",
    "      List[len(List)-1]['base'] = x\n",
    "    else:\n",
    "      continue\n",
    "  return List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "index encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_encoding(s):\n",
    "  List = wordsDicts(s)\n",
    "  onehot = []\n",
    "  for i in range(len(List)):\n",
    "    D = List[i]\n",
    "    onehotbase=  [1]\n",
    "    onehotvm1 =  [1]\n",
    "    onehotvm2 =  [1]\n",
    "    onehotvm3 =  [1]\n",
    "    onehotvm4 =  [1]\n",
    "    onehotcm1 =  [1]\n",
    "    onehotcm2 =  [1]\n",
    "    onehotcm3 =  [1]\n",
    "    onehotcm4 =  [1]\n",
    "\n",
    "\n",
    "    onehotbase[0] = base_mapping[D['base']]\n",
    "\n",
    "    it = 1\n",
    "    if('vm' in D):\n",
    "      for j in D['vm']:\n",
    "        if it == 1:\n",
    "          onehotvm1[0] = vm_mapping[j]\n",
    "        elif it == 2:\n",
    "          onehotvm2[0] = vm_mapping[j]\n",
    "        elif it == 3:\n",
    "          onehotvm3[0] = vm_mapping[j]\n",
    "        elif it == 4:\n",
    "          onehotvm4[0] = vm_mapping[j]\n",
    "        it += 1\n",
    "    \n",
    "    it = 1\n",
    "    if('cm' in D):\n",
    "      for j in D['cm']:\n",
    "        if it == 1:\n",
    "          onehotcm1[0] = cm_mapping[j]\n",
    "        elif it == 2:\n",
    "          onehotcm2[0] = cm_mapping[j]\n",
    "        elif it == 3:\n",
    "          onehotcm3[0] = cm_mapping[j]\n",
    "        elif it == 4:\n",
    "          onehotcm4[0] = cm_mapping[j]\n",
    "        it += 1\n",
    "    onehoti = onehotbase + onehotvm1 + onehotvm2 + onehotvm3 + onehotvm4 + onehotcm1 + onehotcm2 + onehotcm3 + onehotcm4 #size 110 + 4*21 + 4*38 = 346\n",
    "    onehot.append(onehoti)\n",
    "  return torch.tensor(onehot)\n",
    "\n",
    "def index_decoder(List):\n",
    "  x = \"\"\n",
    "  for onehoti in List:\n",
    "    if onehoti[0] > 1:\n",
    "      x += bases[onehoti[0]-2]\n",
    "\n",
    "    if onehoti[5] > 1:\n",
    "      x += connector[0]\n",
    "      x += cms[onehoti[5]-2]\n",
    "    if onehoti[6] > 1:\n",
    "      x += connector[0]\n",
    "      x += cms[onehoti[6]-2]\n",
    "    if onehoti[7] > 1:\n",
    "      x += connector[0]\n",
    "      x += cms[onehoti[7]-2]\n",
    "    if onehoti[8] > 1:\n",
    "      x += connector[0]\n",
    "      x += cms[onehoti[8]-2]\n",
    "\n",
    "    if onehoti[1] > 1:\n",
    "      x += vms[onehoti[1]-2]\n",
    "    if onehoti[2] > 1:\n",
    "      x += vms[onehoti[2]-2]\n",
    "    if onehoti[3] > 1:\n",
    "      x += vms[onehoti[3]-2]\n",
    "    if onehoti[4] > 1:\n",
    "      x += vms[onehoti[4]-2]\n",
    "  return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one-hot-encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(s):\n",
    "  List = wordsDicts(s)\n",
    "  onehot = []\n",
    "  for i in range(len(List)):\n",
    "    D = List[i]\n",
    "    onehotbase=  [0 for _ in range(len(bases) + 2)]\n",
    "    onehotvm1 =  [0 for _ in range(len(vms) + 2)]\n",
    "    onehotvm2 =  [0 for _ in range(len(vms) + 2)]\n",
    "\n",
    "    onehotcm1 =  [0 for _ in range(len(cms) + 2)]   \n",
    "    onehotcm2 =  [0 for _ in range(len(cms) + 2)]   \n",
    "\n",
    "    onehotbase[base_mapping[D['base']]] = 1\n",
    "\n",
    "    it = 1\n",
    "    if('vm' in D):\n",
    "      for j in D['vm']:\n",
    "        if it == 1:\n",
    "          onehotvm1[vm_mapping[j]] = 1\n",
    "        elif it == 2:\n",
    "          onehotvm2[vm_mapping[j]] = 1\n",
    "        it += 1\n",
    "    \n",
    "    it = 1\n",
    "    if('cm' in D):\n",
    "      for j in D['cm']:\n",
    "        if it == 1:\n",
    "          onehotcm1[cm_mapping[j]] = 1\n",
    "        elif it == 2:\n",
    "          onehotcm2[cm_mapping[j]] = 1\n",
    "        it += 1\n",
    "\n",
    "\n",
    "    onehoti = onehotbase + onehotvm1 + onehotvm2 + onehotcm1 + onehotcm2 #size 110 + 4*21 + 4*38 = 346\n",
    "    onehot.append(onehoti)\n",
    "  encoded = torch.tensor(onehot).float().to(device)\n",
    "  return encoded\n",
    "\n",
    "def One_Hot_Decoder(List):\n",
    "  x = \"\"\n",
    "  for onehoti in List:\n",
    "    for i in range(0, 110):\n",
    "      if onehoti[i] == 1 and i > 1:\n",
    "          x += bases[i-2]\n",
    "\n",
    "    for i in range(152, 190):\n",
    "      if onehoti[i] == 1 and i > 152:\n",
    "        x += connector[0]\n",
    "        x += cms[i-153]\n",
    "    for i in range(190, 228):\n",
    "      if onehoti[i] == 1 and i > 191:\n",
    "        x += connector[0]\n",
    "        x += cms[i-192]\n",
    "\n",
    "    for i in range(110, 131):\n",
    "      if onehoti[i] == 1 and i > 111:\n",
    "        x += vms[i-112]\n",
    "    for i in range(131, 152):\n",
    "      if onehoti[i] == 1 and i > 132:\n",
    "        x += vms[i-133]\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def One_Hot_Decoder_Only_Base(List):\n",
    "  x = \"\"\n",
    "  for onehoti in List:\n",
    "    if onehoti[0] == 1:\n",
    "      x += '_'\n",
    "      continue\n",
    "    for i in range(0, 110):\n",
    "      if onehoti[i] == 1 and i > 1:\n",
    "          x += bases[i-2]\n",
    "  return x\n",
    "\n",
    "def One_Hot_Decoder_Only_Vm1(List):\n",
    "  x = \"\"\n",
    "  for onehoti in List:\n",
    "    if onehoti[110] == 1:\n",
    "      x += '_'\n",
    "      continue\n",
    "    if onehoti[111] == 1:\n",
    "      x += ' '\n",
    "      continue\n",
    "    for i in range(110, 131):\n",
    "      if onehoti[i] == 1 and i > 111:\n",
    "        x += vms[i-112]\n",
    "  return x\n",
    "\n",
    "def One_Hot_Decoder_Only_Vm2(List):\n",
    "  x = \"\"\n",
    "  for onehoti in List:\n",
    "    if onehoti[131] == 1:\n",
    "      x += '_'\n",
    "      continue\n",
    "    if onehoti[132] == 1:\n",
    "      x += ' '\n",
    "      continue\n",
    "    for i in range(131, 152):\n",
    "      if onehoti[i] == 1 and i > 132:\n",
    "        x += vms[i-133]\n",
    "  return x\n",
    "\n",
    "\n",
    "def One_Hot_Decoder_Only_Cm1(List):\n",
    "  x = \"\"\n",
    "  for onehoti in List:\n",
    "    if onehoti[152] == 1:\n",
    "      x += '_'\n",
    "      continue\n",
    "    if onehoti[153] == 1:\n",
    "      x += ' '\n",
    "      continue\n",
    "    for i in range(152, 190):\n",
    "      if onehoti[i] == 1 and i > 153:\n",
    "        x += connector[0]\n",
    "        x += cms[i-153]\n",
    "  return x\n",
    "\n",
    "def One_Hot_Decoder_Only_Cm2(List):\n",
    "  x = \"\"\n",
    "  for onehoti in List:\n",
    "    if onehoti[190] == 1:\n",
    "      x += '_'\n",
    "      continue\n",
    "    if onehoti[190] == 1:\n",
    "      x += ' '\n",
    "      continue\n",
    "    for i in range(190, 228):\n",
    "      if onehoti[i] == 1 and i > 191:\n",
    "        x += connector[0]\n",
    "        x += cms[i-192]\n",
    "  return x\n",
    "\n",
    "def index_decoder_Only_Base(List):\n",
    "  x = \"\"\n",
    "  for onehoti in List:\n",
    "    if onehoti[0] > 1:\n",
    "      x += bases[onehoti[0]-2]\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = EncoderCNN().to(device)\n",
    "rnn = DECODER_RNN().to(device)\n",
    "\n",
    "cnn.load_state_dict(torch.load(\"/home/ocr/teluguOCR/Models/Best_CNN/GRU1/Model5.pth\"))\n",
    "rnn.load_state_dict(torch.load(\"/home/ocr/teluguOCR/Models/Best_RNN/GRU1/Model5.pth\"))\n",
    "\n",
    "cnn.eval()\n",
    "rnn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a JPEG image an convert to tensor\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "\n",
    "img = Image.open(\"/home/ocr/teluguOCR/test7.png\")\n",
    "img = img.convert('L')\n",
    "\n",
    "# Denoising the image using Gaussian filter\n",
    "img = cv.fastNlMeansDenoising(np.array(img), None, 10, 7, 21)\n",
    "\n",
    "img = transforms.ToTensor()(img)\n",
    "# img = image_tensor[0, 610:690, 160:405]\n",
    "\n",
    "print(img.shape)\n",
    "print(torch.max(img), torch.min(img))\n",
    "img *= 255\n",
    "\n",
    "img = Image.fromarray(np.array(img[0], dtype=np.uint8))\n",
    "\n",
    "m = img.size[1]//40\n",
    "\n",
    "img = img.resize((img.size[0]//m, 40))\n",
    "\n",
    "img = transforms.ToTensor()(img)\n",
    "img [img< 0] = 0\n",
    "img [img> 255] = 255\n",
    "\n",
    "img*= 255\n",
    "\n",
    "img = 255 - img\n",
    "\n",
    "th = 100\n",
    "img[img > th] = 255\n",
    "# img[img < th] = 0\n",
    "\n",
    "f_img = torch.zeros(40, 800)\n",
    "\n",
    "f_img[:, 0:img.shape[2]] = img[0]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(f_img[:, :], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_out = cnn(f_img.unsqueeze(0).unsqueeze(0).to(device)).unsqueeze(1)\n",
    "\n",
    "f_out = rnn(cnn_out)\n",
    "\n",
    "f_out[:, :, 0:110] = F.softmax(f_out[:, :, 0:110], dim=2)\n",
    "\n",
    "f_out[:, :, 110:131] = F.softmax(f_out[:, :, 110:131], dim=2)\n",
    "f_out[:, :, 131:152] = F.softmax(f_out[:, :, 131:152], dim=2)\n",
    "\n",
    "f_out[:, :, 152:190] = F.softmax(f_out[:, :, 152:190], dim=2)\n",
    "f_out[:, :, 190:229] = F.softmax(f_out[:, :, 190:228], dim=2)\n",
    "\n",
    "f_out = f_out.squeeze(1)\n",
    "\n",
    "final_label = torch.zeros(Image_length, Text_embedding_size).to(device)\n",
    "\n",
    "for i in range(Image_length):\n",
    "    final_label[i, torch.argmax(f_out[i, :110])] = 1\n",
    "    final_label[i, 110 + torch.argmax(f_out[i, 110:131])] = 1\n",
    "    final_label[i, 131 + torch.argmax(f_out[i, 131:152])] = 1\n",
    "    final_label[i, 152 + torch.argmax(f_out[i, 152:190])] = 1\n",
    "    final_label[i, 190 + torch.argmax(f_out[i, 190:228])] = 1\n",
    "\n",
    "\n",
    "print(\"predicted label: \", One_Hot_Decoder(final_label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    if(final_label[i][0] == 1):\n",
    "        print('_', end = '')\n",
    "    else:\n",
    "        print(One_Hot_Decoder_Only_Base(final_label[i][:110].unsqueeze(0)), end = '')\n",
    "print()\n",
    "\n",
    "base = []\n",
    "vm1 = []\n",
    "vm2 = []\n",
    "cm1 = []\n",
    "cm2 = []\n",
    "\n",
    "for i in range(100):\n",
    "    base.append(One_Hot_Decoder_Only_Base(final_label[i].unsqueeze(0)))\n",
    "    vm1.append(One_Hot_Decoder_Only_Vm1(final_label[i].unsqueeze(0)))\n",
    "    vm2.append(One_Hot_Decoder_Only_Vm2(final_label[i].unsqueeze(0)))\n",
    "    cm1.append(One_Hot_Decoder_Only_Cm1(final_label[i].unsqueeze(0)))\n",
    "    cm2.append(One_Hot_Decoder_Only_Cm2(final_label[i].unsqueeze(0)))\n",
    "\n",
    "# print(base)\n",
    "# print(vm1)\n",
    "# print(vm2)\n",
    "# print(vm3)\n",
    "# print(vm4)\n",
    "# print(cm1)\n",
    "# print(cm2)\n",
    "# print(cm3)\n",
    "# print(cm4)\n",
    "\n",
    "# Removing the continuous duplicate occurances of the same character in the Lists\n",
    "base = [base[i] for i in range(len(base)) if i == 0 or base[i] != base[i-1]]\n",
    "vm1 = [vm1[i] for i in range(len(vm1)) if i == 0 or vm1[i] != vm1[i-1]]\n",
    "vm2 = [vm2[i] for i in range(len(vm2)) if i == 0 or vm2[i] != vm2[i-1]]\n",
    "cm1 = [cm1[i] for i in range(len(cm1)) if i == 0 or cm1[i] != cm1[i-1]]\n",
    "cm2 = [cm2[i] for i in range(len(cm2)) if i == 0 or cm2[i] != cm2[i-1]]\n",
    "\n",
    "# removing all occurances fo '' in the Lists\n",
    "base = [x for x in base if x != '_']\n",
    "vm1 = [x for x in vm1 if x != '_']\n",
    "vm2 = [x for x in vm2 if x != '_']\n",
    "cm1 = [x for x in cm1 if x != '_']\n",
    "cm2 = [x for x in cm2 if x != '_']\n",
    "\n",
    "print(len(base), \" | \", base)\n",
    "print(len(vm1), \" | \", vm1)\n",
    "print(len(vm2), \" | \", vm2)\n",
    "print(len(cm1), \" | \", cm1)\n",
    "print(len(cm2), \" | \", cm2)\n",
    "\n",
    "x = \"\"\n",
    "for i in range(len(base)):\n",
    "    x += base[i]\n",
    "    if base[i] == ' ':\n",
    "      continue\n",
    "\n",
    "    if i < len(cm1) and  cm1[i] != ' ':\n",
    "      x += cm1[i]\n",
    "    if i < len(cm2) and  cm2[i] != ' ':\n",
    "      x += cm2[i]\n",
    "\n",
    "    if i < len(vm1) and  vm1[i] != ' ':\n",
    "      x += vm1[i]\n",
    "    if i < len(vm2) and  vm2[i] != ' ':\n",
    "      x += vm2[i]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(f_img[:, :], cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image from data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_num = random.randint(1, 150854 + 1)\n",
    "\n",
    "from dataset import *\n",
    "\n",
    "data = TeluguOCRDataset(\"/home/ocr/teluguOCR/Dataset/Cropped_Dataset/Images\", \"/home/ocr/teluguOCR/Dataset/Cropped_Dataset/Labels\")\n",
    "\n",
    "data_point = data.__getitem__(image_num)\n",
    "\n",
    "image = data_point[0]\n",
    "label = data_point[1]\n",
    "\n",
    "image = image.to(device)\n",
    "label = label.to(device)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(image.squeeze(0).squeeze(0).cpu(), cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "label = label.squeeze(0).cpu().to(torch.int64).numpy()\n",
    "\n",
    "print(\"actual label: \", index_decoder(label))\n",
    "\n",
    "image = image.unsqueeze(0).to(device)\n",
    "\n",
    "cnn_output = cnn(image).unsqueeze(1)\n",
    "\n",
    "\n",
    "f_output = rnn(cnn_output)\n",
    "\n",
    "f_output[:, :, 0:110] = F.softmax(f_output[:, :, 0:110], dim=2)\n",
    "\n",
    "f_output[:, :, 110:131] = F.softmax(f_output[:, :, 110:131], dim=2)\n",
    "f_output[:, :, 131:152] = F.softmax(f_output[:, :, 131:152], dim=2)\n",
    "\n",
    "f_output[:, :, 152:190] = F.softmax(f_output[:, :, 152:190], dim=2)\n",
    "f_output[:, :, 190:228] = F.softmax(f_output[:, :, 190:228], dim=2)\n",
    "\n",
    "f_output = f_output.squeeze(1)\n",
    "\n",
    "final_label = torch.zeros(Image_length, Text_embedding_size).to(device)\n",
    "\n",
    "for i in range(Image_length):\n",
    "    final_label[i, torch.argmax(f_output[i, :110])] = 1\n",
    "\n",
    "    final_label[i, torch.argmax(f_output[i, 110:131])+110] = 1\n",
    "    final_label[i, torch.argmax(f_output[i, 131:152])+131] = 1\n",
    "\n",
    "    final_label[i, torch.argmax(f_output[i, 152:190])+152] = 1\n",
    "    final_label[i, torch.argmax(f_output[i, 190:228])+190] = 1\n",
    "\n",
    "print(\"predicted label: \", One_Hot_Decoder(final_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the range values in f_output\n",
    "print(torch.max(f_output[0, :110]))\n",
    "print(torch.min(f_output[0, :110]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(image.squeeze(0).squeeze(0).cpu(), cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "print(\"   Actual label: \", index_decoder(label))\n",
    "\n",
    "# for i in range(100):\n",
    "#     if(final_label[i][0] == 1):\n",
    "#         print('_', end = '')\n",
    "#     else:\n",
    "#         print(One_Hot_Decoder_Only_Base(final_label[i][:110].unsqueeze(0)), end = '')\n",
    "# print()\n",
    "\n",
    "base = []\n",
    "vm1 = []\n",
    "vm2 = []\n",
    "vm3 = []\n",
    "vm4 = []\n",
    "cm1 = []\n",
    "cm2 = []\n",
    "cm3 = []\n",
    "cm4 = []\n",
    "\n",
    "for i in range(100):\n",
    "    base.append(One_Hot_Decoder_Only_Base(final_label[i].unsqueeze(0)))\n",
    "    vm1.append(One_Hot_Decoder_Only_Vm1(final_label[i].unsqueeze(0)))\n",
    "    vm2.append(One_Hot_Decoder_Only_Vm2(final_label[i].unsqueeze(0)))\n",
    "    cm1.append(One_Hot_Decoder_Only_Cm1(final_label[i].unsqueeze(0)))\n",
    "    cm2.append(One_Hot_Decoder_Only_Cm2(final_label[i].unsqueeze(0)))\n",
    "\n",
    "# print(base)\n",
    "# print(vm1)\n",
    "# print(vm2)\n",
    "# print(vm3)\n",
    "# print(vm4)\n",
    "# print(cm1)\n",
    "# print(cm2)\n",
    "# print(cm3)\n",
    "# print(cm4)\n",
    "\n",
    "# Removing the continuous duplicate occurances of the same character in the Lists\n",
    "base = [base[i] for i in range(len(base)) if i == 0 or base[i] != base[i-1]]\n",
    "vm1 = [vm1[i] for i in range(len(vm1)) if i == 0 or vm1[i] != vm1[i-1]]\n",
    "vm2 = [vm2[i] for i in range(len(vm2)) if i == 0 or vm2[i] != vm2[i-1]]\n",
    "cm1 = [cm1[i] for i in range(len(cm1)) if i == 0 or cm1[i] != cm1[i-1]]\n",
    "cm2 = [cm2[i] for i in range(len(cm2)) if i == 0 or cm2[i] != cm2[i-1]]\n",
    "\n",
    "# removing all occurances fo '' in the Lists\n",
    "base = [x for x in base if x != '_']\n",
    "vm1 = [x for x in vm1 if x != '_']\n",
    "vm2 = [x for x in vm2 if x != '_']\n",
    "cm1 = [x for x in cm1 if x != '_']\n",
    "cm2 = [x for x in cm2 if x != '_']\n",
    "\n",
    "x = \"\"\n",
    "for i in range(len(base)):\n",
    "    x += base[i]\n",
    "    if base[i] == ' ':\n",
    "      continue\n",
    "\n",
    "    if i < len(cm1) and  cm1[i] != ' ':\n",
    "      x += cm1[i]\n",
    "    if i < len(cm2) and  cm2[i] != ' ':\n",
    "      x += cm2[i]\n",
    "\n",
    "    if i < len(vm1) and  vm1[i] != ' ':\n",
    "      x += vm1[i]\n",
    "    if i < len(vm2) and  vm2[i] != ' ':\n",
    "      x += vm2[i]\n",
    "      \n",
    "print(\"Predicted Label: \", x, end = '\\n\\n')\n",
    "\n",
    "print(len(base), \" | \", base)\n",
    "print(len(vm1), \" | \", vm1)\n",
    "print(len(vm2), \" | \", vm2)\n",
    "print(len(cm1), \" | \", cm1)\n",
    "print(len(cm2), \" | \", cm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(GTT, FP):\n",
    "    GTT = GTT\n",
    "    FP = FP\n",
    "    \n",
    "    T = len(wordsDicts(index_decoder(GTT)))\n",
    "    B = 0\n",
    "    V = 0\n",
    "    C = 0\n",
    "    for i in range(min(T, len(FP))):\n",
    "        if GTT[i][0] == FP[i][0]:\n",
    "                B += 1\n",
    "        if GTT[i][1] == FP[i][1]:\n",
    "                V += 1\n",
    "        if GTT[i][2] == FP[i][2]:\n",
    "                V += 1\n",
    "        if GTT[i][5] == FP[i][5]:\n",
    "                C += 1\n",
    "        if GTT[i][6] == FP[i][6]:\n",
    "                C += 1\n",
    "    return T, B, C, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_acc(cnn, rnn, image, label):\n",
    "    label = label.squeeze(0).cpu().to(torch.int64).numpy()\n",
    "    image = image.to(device)\n",
    "    cnn_output = cnn(image).unsqueeze(1)\n",
    "    f_output = rnn(cnn_output)\n",
    "\n",
    "    f_output[:, :, 0:110] = F.softmax(f_output[:, :, 0:110], dim=2)\n",
    "    f_output[:, :, 110:131] = F.softmax(f_output[:, :, 110:131], dim=2)\n",
    "    f_output[:, :, 131:152] = F.softmax(f_output[:, :, 131:152], dim=2)\n",
    "    f_output[:, :, 152:190] = F.softmax(f_output[:, :, 152:190], dim=2)\n",
    "    f_output[:, :, 190:228] = F.softmax(f_output[:, :, 190:228], dim=2)\n",
    "\n",
    "    f_output = f_output.squeeze(1)\n",
    "    final_label = torch.zeros(Image_length, Text_embedding_size).to(device)\n",
    "\n",
    "    for i in range(Image_length):\n",
    "        final_label[i, torch.argmax(f_output[i, :110])] = 1\n",
    "        final_label[i, torch.argmax(f_output[i, 110:131])+110] = 1\n",
    "        final_label[i, torch.argmax(f_output[i, 131:152])+131] = 1\n",
    "        final_label[i, torch.argmax(f_output[i, 152:190])+152] = 1\n",
    "        final_label[i, torch.argmax(f_output[i, 190:228])+190] = 1\n",
    "\n",
    "    base = []\n",
    "    vm1 = []\n",
    "    vm2 = []\n",
    "    cm1 = []\n",
    "    cm2 = []\n",
    "\n",
    "    for i in range(100):\n",
    "        base.append(One_Hot_Decoder_Only_Base(final_label[i].unsqueeze(0)))\n",
    "        vm1.append(One_Hot_Decoder_Only_Vm1(final_label[i].unsqueeze(0)))\n",
    "        vm2.append(One_Hot_Decoder_Only_Vm2(final_label[i].unsqueeze(0)))\n",
    "        cm1.append(One_Hot_Decoder_Only_Cm1(final_label[i].unsqueeze(0)))\n",
    "        cm2.append(One_Hot_Decoder_Only_Cm2(final_label[i].unsqueeze(0)))\n",
    "\n",
    "    # Removing the continuous duplicate occurances of the same character in the Lists\n",
    "    base = [base[i] for i in range(len(base)) if i == 0 or base[i] != base[i-1]]\n",
    "    vm1 = [vm1[i] for i in range(len(vm1)) if i == 0 or vm1[i] != vm1[i-1]]\n",
    "    vm2 = [vm2[i] for i in range(len(vm2)) if i == 0 or vm2[i] != vm2[i-1]]\n",
    "    cm1 = [cm1[i] for i in range(len(cm1)) if i == 0 or cm1[i] != cm1[i-1]]\n",
    "    cm2 = [cm2[i] for i in range(len(cm2)) if i == 0 or cm2[i] != cm2[i-1]]\n",
    "\n",
    "    # removing all occurances fo '' in the Lists\n",
    "    base = [x for x in base if x != '_']\n",
    "    vm1 = [x for x in vm1 if x != '_']\n",
    "    vm2 = [x for x in vm2 if x != '_']\n",
    "    cm1 = [x for x in cm1 if x != '_']\n",
    "    cm2 = [x for x in cm2 if x != '_']\n",
    "    \n",
    "    x = \"\"\n",
    "    for i in range(len(base)):\n",
    "        x += base[i]\n",
    "        if base[i] == ' ':\n",
    "            continue\n",
    "\n",
    "        if i < len(cm1) and  cm1[i] != ' ':\n",
    "            x += cm1[i]\n",
    "        if i < len(cm2) and  cm2[i] != ' ':\n",
    "            x += cm2[i]\n",
    "\n",
    "        if i < len(vm1) and  vm1[i] != ' ':\n",
    "            x += vm1[i]\n",
    "        if i < len(vm2) and  vm2[i] != ' ':\n",
    "            x += vm2[i]\n",
    "    \n",
    "    T, B, C, V = get_accuracy(label, index_encoding(x))\n",
    "    acc = (0.5*B + 0.3*(C//2) + 0.2*(V//2))/T\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.7687097757353727\n",
      "Validation Accuracy:  0.7638860313946141\n"
     ]
    }
   ],
   "source": [
    "from dataset import *\n",
    "dataset = TeluguOCRDataset(\"/home/ocr/teluguOCR/Dataset/Cropped_Dataset/Images\", \"/home/ocr/teluguOCR/Dataset/Cropped_Dataset/Labels\")\n",
    "\n",
    "# splitting the dataset into training and validation\n",
    "torch.manual_seed(0)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [0.7, 0.3])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "cnn = EncoderCNN().to(device)\n",
    "rnn = DECODER_RNN().to(device)\n",
    "\n",
    "cnn.load_state_dict(torch.load(\"/home/ocr/teluguOCR/Models/Best_CNN/LSTM/Model5.pth\"))\n",
    "rnn.load_state_dict(torch.load(\"/home/ocr/teluguOCR/Models/Best_RNN/LSTM/Model5.pth\"))\n",
    "\n",
    "# train accuracy\n",
    "acc = 0\n",
    "total_num = 0\n",
    "for image, label, _, _ in train_dataloader:\n",
    "    acc += calculate_acc(cnn, rnn, image, label)\n",
    "    total_num += 1\n",
    "    if total_num == 1000:\n",
    "        break\n",
    "    print(\"T: \", total_num, end = '\\r')\n",
    "print(\"Train Accuracy: \", (acc/total_num))\n",
    "\n",
    "# validation accuracy\n",
    "acc = 0\n",
    "total_num = 0\n",
    "for image, label, _, _ in val_dataloader:\n",
    "    acc += calculate_acc(cnn, rnn, image, label)\n",
    "    total_num += 1\n",
    "    if total_num == 1000:\n",
    "        break\n",
    "    print(\"V: \", total_num, end = '\\r')\n",
    "print(\"Validation Accuracy: \", (acc/total_num))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teluguOCR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
