{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3822,"status":"ok","timestamp":1699864430989,"user":{"displayName":"MULUGU VISHWANATH SHARMA","userId":"10455501007410540821"},"user_tz":-330},"id":"41jpf_fwowrj","outputId":"25eab520-4821-4b0a-bb07-be75caefeee3"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import time\n","import torchsummary as summary\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# device = torch.device(\"cpu\")"]},{"cell_type":"markdown","metadata":{},"source":["Hyper parameters of the whole structure"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# out look of the model\n","Image_size = (30, 600) # (height, width)\n","Image_embedding_size = 100\n","image_length = 418\n","Text_embedding_size = 364\n","Max_Number_of_Words = 350\n","\n","# Joiner Embedder parameters\n","Joiner_Input_size = Image_embedding_size #364\n","Joiner_output_size = Image_embedding_size #364\n","\n","# LSTM parameters for the RNN\n","LSTM_Input_size = Joiner_output_size #364\n","LSTM_hidden_size = LSTM_Input_size #364\n","LSTM_num_layers = 1\n","LSTM_output_size = LSTM_hidden_size #364\n","\n","# reverse Embedding parameters\n","Reverse_Input_size = LSTM_output_size #364\n","Reverse_output_size = Text_embedding_size #364\n","\n","drop_prob = 0.3"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["acchulu = ['అ', 'ఆ', 'ఇ', 'ఈ', 'ఉ', 'ఊ', 'ఋ', 'ౠ', 'ఌ', 'ౡ', 'ఎ', 'ఏ', 'ఐ', 'ఒ', 'ఓ', 'ఔ', 'అం', 'అః']\n","hallulu = ['క', 'ఖ', 'గ', 'ఘ', 'ఙ',\n","           'చ', 'ఛ', 'జ', 'ఝ', 'ఞ',\n","           'ట', 'ఠ', 'డ', 'ఢ', 'ణ',\n","           'త', 'థ', 'ద', 'ధ', 'న',\n","           'ప', 'ఫ', 'బ', 'భ', 'మ',\n","           'య', 'ర', 'ల', 'వ', 'శ', 'ష', 'స', 'హ', 'ళ', 'క్ష', 'ఱ', 'ఴ', 'ౘ', 'ౙ','ౚ']\n","vallulu = ['ా', 'ి', 'ీ', 'ు' , 'ూ', 'ృ', 'ౄ', 'ె', 'ే', 'ై', 'ొ', 'ో', 'ౌ', 'ం', 'ః', 'ఁ', 'ఀ', 'ఄ', 'ౕ', 'ౖ', 'ౢ' ]\n","connector = ['్']\n","numbers = ['౦', '౧', '౨', '౩', '౪', '౫', '౬', '౭', '౮', '౯']\n","splcharacters= [' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')',\n","              '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[',\n","              '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '1','2', '3', '4', '5', '6', '7', '8', '9', '0', 'ఽ']\n","spl = splcharacters + numbers\n","\n","bases = acchulu + hallulu + spl\n","vms = vallulu\n","cms = hallulu\n","\n","characters = bases+vms+cms+connector\n","\n","base_mapping = {}\n","i = 1\n","for x in bases:\n","  base_mapping[x] = i\n","  i+=1\n","\n","vm_mapping = {}\n","i = 1\n","for x in vms:\n","  vm_mapping[x] = i\n","  i+=1\n","\n","cm_mapping = {}\n","i = 1\n","for x in cms:\n","  cm_mapping[x] = i\n","  i+=1\n","\n","# creates a list of ductionaries with each dictionary reporesenting a term\n","def wordsDicts(s):\n","  List = []\n","  for i in range(len(s)):\n","    x = s[i]\n","    prev = ''\n","    if i > 0: prev = s[i-1]\n","    #----------------------------------is it a base term-----------------------\n","    if((x in acchulu or x in hallulu)  and prev != connector[0]):\n","      List.append({})\n","      List[-1]['base'] = x\n","    #----------------------------if it is a consonant modifier-----------------\n","    elif x in hallulu and prev == connector[0]:\n","      if(len(List) == 0):\n","        print(x)\n","      if('cm' not in List[-1]): List[-1]['cm'] = []\n","      List[len(List)-1]['cm'].append(x)\n","\n","      #---------------------------if it is a vowel modifier--------------------\n","    elif x in vallulu:\n","      if(len(List) == 0):\n","        print(x)\n","\n","      if('vm' not in List[-1]): List[-1]['vm'] = []\n","      List[len(List)-1]['vm'].append(x)\n","\n","      #----------------------------it is a spl character-----------------------\n","    elif x in spl:\n","      List.append({})\n","      List[len(List)-1]['base'] = x\n","    else:\n","      continue\n","  return List"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def index_encoding(s):\n","  List = wordsDicts(s)\n","  onehot = []\n","  for i in range(len(List)):\n","    D = List[i]\n","    onehotbase=  [0]\n","    onehotvm1 =  [1]\n","    onehotvm2 =  [1]\n","    onehotvm3 =  [1]\n","    onehotvm4 =  [1]\n","    onehotcm1 =  [1]\n","    onehotcm2 =  [1]\n","    onehotcm3 =  [1]\n","    onehotcm4 =  [1]\n","\n","\n","    onehotbase[0] = base_mapping[D['base']]\n","\n","    it = 1\n","    if('vm' in D):\n","      for j in D['vm']:\n","        if it == 1:\n","          onehotvm1[0] = vm_mapping[j]+1\n","        elif it == 2:\n","          onehotvm2[0] = vm_mapping[j]+1\n","        elif it == 3:\n","          onehotvm3[0] = vm_mapping[j]+1\n","        elif it == 4:\n","          onehotvm4[0] = vm_mapping[j]+1\n","        it += 1\n","    \n","    it = 1\n","    if('cm' in D):\n","      for j in D['cm']:\n","        if it == 1:\n","          onehotcm1[0] = cm_mapping[j]+1\n","        elif it == 2:\n","          onehotcm2[0] = cm_mapping[j]+1\n","        elif it == 3:\n","          onehotcm3[0] = cm_mapping[j]+1\n","        elif it == 4:\n","          onehotcm4[0] = cm_mapping[j]+1\n","        it += 1\n","    onehoti = onehotbase + onehotvm1 + onehotvm2 + onehotvm3 + onehotvm4 + onehotcm1 + onehotcm2 + onehotcm3 + onehotcm4 #size 112 + 4*21 + 4*40 = 356\n","    onehot.append(onehoti)\n","  return onehot\n","\n","def index_decoder(List):\n","  x = \"\"\n","  for onehoti in List:\n","    x += bases[onehoti[0]-1]\n","\n","    if onehoti[5] != 1:\n","      x += connector[0]\n","      x += cms[onehoti[5]-2]\n","    if onehoti[6] != 1:\n","      x += connector[0]\n","      x += cms[onehoti[6]-2]\n","    if onehoti[7] != 1:\n","      x += connector[0]\n","      x += cms[onehoti[7]-2]\n","    if onehoti[8] != 1:\n","      x += connector[0]\n","      x += cms[onehoti[8]-2]\n","\n","    if onehoti[1] != 1:\n","      x += vms[onehoti[1]-2]\n","    if onehoti[2] != 1:\n","      x += vms[onehoti[2]-2]\n","    if onehoti[3] != 1:\n","      x += vms[onehoti[3]-2]\n","    if onehoti[4] != 1:\n","      x += vms[onehoti[4]-2]\n","  return x"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","class EncoderCNN(nn.Module):\n","    def __init__(self) -> None:\n","        super(EncoderCNN, self).__init__()\n","        # input: 30 x 600  output: 364 x 300\n","        self.convSequence = nn.Sequential(\n","            nn.Conv2d(1, 32, kernel_size=(5, 15), stride=(1, 1), padding=(2, 0)), \n","            nn.Conv2d(32, 32, kernel_size=(5, 15), stride=(1, 1), padding=(2, 0)), \n","            nn.BatchNorm2d(32),\n","\n","            nn.Conv2d(32, 64, kernel_size=(5, 15), stride=(1, 1), padding=(2, 0)),\n","            nn.Conv2d(64, 64, kernel_size=(5, 15), stride=(1, 1), padding=(0, 0)),\n","            nn.BatchNorm2d(64),\n","\n","            nn.Conv2d(64, 128, kernel_size=(5, 15), stride=(1, 1), padding=(2, 0)),\n","            nn.Conv2d(128, 128, kernel_size=(5, 15), stride=(1, 1), padding=(2, 0)),\n","            nn.Conv2d(128, 128, kernel_size=(5, 15), stride=(1, 1), padding=(0, 0)),\n","            nn.BatchNorm2d(128),\n","\n","            nn.Conv2d(128, 256, kernel_size=(5, 15), stride=(1, 1), padding=(0, 0)),\n","            nn.Conv2d(256, 256, kernel_size=(5, 15), stride=(1, 1), padding=(0, 0)),\n","            nn.Conv2d(256, 256, kernel_size=(5, 15), stride=(1, 1), padding=(0, 0)),\n","            nn.BatchNorm2d(256),\n","\n","            nn.Conv2d(256, 512, kernel_size=(5, 15), stride=(1, 1), padding=(0, 0)),\n","            nn.Conv2d(512, 512, kernel_size=(5, 15), stride=(1, 1), padding=(0, 0)),\n","            nn.Conv2d(512, 512, kernel_size=(2, 15), stride=(1, 1), padding=(0, 0)),\n","            nn.BatchNorm2d(512),\n","        )\n","\n","        self.GlobalMaxPool = nn.AdaptiveMaxPool2d((1, None))\n","\n","        self.fc = nn.Linear(512, Image_embedding_size)\n","\n","\n","    def forward(self, x):\n","        x = self.convSequence(x)\n","        x = self.GlobalMaxPool(x)\n","\n","        x = x.squeeze(2)\n","        x = x.permute(0, 2, 1)\n","        x = self.fc(x)\n","        x = x.permute(0, 2, 1)\n","\n","\n","        return x\n","    \n","# cnn = EncoderCNN().to(device)\n","# temp = torch.randn(20, 1, 30, 600).to(device)\n","# print(cnn(temp).shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# summary.summary(cnn, (1, 30, 600))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class LSTM_Net(nn.Module):\n","    def __init__(self) -> None:\n","        super(LSTM_Net, self).__init__()\n","        # embedding layer sizes\n","        self.einput_size = Joiner_Input_size #\n","        self.eoutput_size = Joiner_output_size #200\n","        # LSTM parameters\n","        self.embed_size = LSTM_Input_size #200\n","        self.hidden_size = LSTM_hidden_size #200\n","        self.num_layers = LSTM_num_layers #1\n","        # reverse embedding layer sizes\n","        self.Rinput_size = Reverse_Input_size #200\n","        self.Routput_size = Reverse_output_size #364\n","\n","        # dense embedding layers from 50 to 200\n","        self.embedding1 = nn.Linear(self.einput_size, self.eoutput_size, bias=False)\n","        \n","        # LSTM layer\n","        self.lstm1 = nn.LSTM(input_size = self.embed_size, hidden_size = int(self.embed_size/2) , num_layers = self.num_layers, bidirectional = True, batch_first=True, dropout = drop_prob) #200 to 200\n","        self.lstm2 = nn.LSTM(input_size = self.embed_size, hidden_size = int(self.embed_size/2), num_layers = self.num_layers, bidirectional = True, batch_first=True, dropout = drop_prob) #200 to 200\n","        self.lstm3 = nn.LSTM(input_size = self.embed_size, hidden_size = int(self.embed_size/2) , num_layers = self.num_layers, bidirectional = True, batch_first=True, dropout = drop_prob) #200 to 200\n","\n","        # attention layers for the LSTM\n","        self.attention_Q = nn.Linear(self.Rinput_size, self.Rinput_size)\n","        self.attention_K = nn.Linear(self.Rinput_size, self.Rinput_size)\n","        self.attention_V = nn.Linear(self.Rinput_size, self.Rinput_size)\n","\n","        # dense layers from 200 to 364\n","        self.Dense1 = nn.Linear(self.Rinput_size, self.Routput_size, bias=False)\n","        \n","        # initialise the weights of the embedding layers\n","        self.relu = nn.ReLU()\n","         \n","    def init_hidden(self, batch_size):\n","        self.hidden1 = (torch.zeros(2*self.num_layers, batch_size, int(self.embed_size/2)).to(device),\n","                torch.zeros(2*self.num_layers, batch_size, int(self.embed_size/2)).to(device))\n","\n","        self.hidden2 = (torch.zeros(2*self.num_layers, batch_size, int(self.embed_size/2)).to(device),\n","                torch.zeros(2*self.num_layers, batch_size, int(self.embed_size/2)).to(device))\n","\n","        self.hidden3 = (torch.zeros(2*self.num_layers, batch_size, int(self.embed_size/2)).to(device),\n","                torch.zeros(2*self.num_layers, batch_size, int(self.embed_size/2)).to(device))\n","        \n","\n","    def forward(self, input, New = False):\n","        if New: # if the input is the image embedding then reset the hidden layers to zeros.\n","            self.init_hidden(input.shape[0])\n","\n","        input = self.embedding1(input) # 358 to 17500 \n","            \n","        # LSTM layers\n","        output, self.hidden1 = self.lstm1(input, self.hidden1)\n","        output, self.hidden2 = self.lstm2(output, self.hidden2)\n","        output, self.hidden3 = self.lstm3(output, self.hidden3)\n","\n","        # attention layer\n","        Q = self.attention_Q(output)\n","        K = self.attention_K(output)\n","        V = self.attention_V(output)\n","        attention = torch.bmm(Q, K.transpose(1, 2))\n","        attention = F.softmax(attention, dim=2)\n","        attention = torch.bmm(attention, V)\n","        \n","        # dense layer\n","        attention = F.relu(attention)\n","        attention = self.Dense1(attention)\n","\n","        return attention"]},{"cell_type":"markdown","metadata":{},"source":["Training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Losses = []"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cnn = EncoderCNN().to(device)\n","network = LSTM_Net().to(device)\n","\n","cnn.load_state_dict(torch.load(\"../Saved_Models/Model_cnn3.pth\"))\n","network.load_state_dict(torch.load(\"../Saved_Models/Model_rnn3.pth\"))\n","\n","cnn.train()\n","network.train()\n","\n","params = list(network.parameters()) + list(cnn.parameters())\n","optimizer = optim.Adam(params, lr=1e-3)\n","\n","# gradient clipping\n","clip = 1.0\n","torch.nn.utils.clip_grad_norm_(params, clip, norm_type=2, error_if_nonfinite=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def read_file_lines(filename):\n","    lines = []\n","    try:\n","        with open(filename, 'r') as file:\n","            for line in file:\n","                lines.append(line.strip())  # Remove trailing newline characters\n","    except FileNotFoundError:\n","        print(f\"File '{filename}' not found.\")\n","    except Exception as e:\n","        print(f\"An error occurred: {str(e)}\")\n","    return lines"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["critereon = nn.CTCLoss(blank=0).cuda() if torch.cuda.is_available() else nn.CTCLoss(blank=0)\n","\n","\n","num_of_epochs = 300\n","\n","files = 1\n","\n","for Epoch in range(1, num_of_epochs + 1):\n","\n","    Epoch_loss = 0\n","    start = time.time()\n","    number_of_images = 0\n","\n","\n","    for i in range(1, 41):\n","        print(\"file: \", i, end = \"\\r\")\n","\n","        image = torch.load(\"../Dataset/Full_Image_Tensors/Full_Image_Tensors\" + str(i) + \".pt\").to(device)\n","        label = torch.load(\"../Dataset/Full_Label_Tensors/Full_Label_Tensors\" + str(i) + \".pt\").to(device)\n","        target_lengths = torch.load(\"../Dataset/Full_label_length_tensors/Full_Label_Lengths\" + str(i) + \".pt\").to(device)\n","\n","        # randomly selecting 20 images from the batch\n","        random_indices = torch.randperm(image.shape[0])[:20]\n","\n","        image = image[random_indices]\n","        label = label[random_indices]\n","        target_lengths = target_lengths[random_indices]\n","\n","\n","        num_of_batches = image.shape[1]\n","        num_of_images = image.shape[0]\n","        \n","        for sub_epochs in range(1, 2):\n","            # print(\"sub_epoch: \", sub_epochs)\n","            # CNN Model\n","\n","            cnn_output = cnn(image).unsqueeze(1)\n","\n","            # LSTM Model\n","            f_out = torch.zeros(num_of_images, Text_embedding_size, image_length).to(device)\n","            for k in range(image_length):\n","                f_out[:, :, k] = network(cnn_output[:, :, :, k], k == 0).squeeze(1)\n","            f_out = f_out.permute(1, 0, 2)\n","\n","            # softmaxing the output\n","            f_out[:, :, :112] = F.log_softmax(f_out[:, :, :112], dim=2)\n","            f_out[:, :, 112:134] = F.log_softmax(f_out[:, :, 112:134], dim=2)\n","            f_out[:, :, 134:156] = F.log_softmax(f_out[:, :, 134:156], dim=2)\n","            f_out[:, :, 156:178] = F.log_softmax(f_out[:, :, 156:178], dim=2)\n","            f_out[:, :, 178:200] = F.log_softmax(f_out[:, :, 178:200], dim=2)\n","            f_out[:, :, 200:241] = F.log_softmax(f_out[:, :, 200:241], dim=2)\n","            f_out[:, :, 241:282] = F.log_softmax(f_out[:, :, 241:282], dim=2)\n","            f_out[:, :, 282:323] = F.log_softmax(f_out[:, :, 282:323], dim=2)\n","            f_out[:, :, 323:364] = F.log_softmax(f_out[:, :, 323:364], dim=2)\n","            \n","            loss = 0\n","\n","            input_lengths = torch.full(size=(f_out.shape[1],), fill_value=f_out.shape[0], dtype=torch.long).to(device)\n","\n","            # for base            \n","            loss += critereon(f_out[:, :, :112], label[:, :, 0], input_lengths, target_lengths)\n","            # for vm1\n","            loss += critereon(f_out[:, :, 112:134], label[:, :, 1], input_lengths, target_lengths)\n","            # for vm2\n","            loss += critereon(f_out[:, :, 134:156], label[:, :, 2], input_lengths, target_lengths)\n","            # for vm3\n","            loss += critereon(f_out[:, :, 156:178], label[:, :, 3], input_lengths, target_lengths)\n","            # for vm4\n","            loss += critereon(f_out[:, :, 178:200], label[:, :, 4], input_lengths, target_lengths)\n","            # for cm1\n","            loss += critereon(f_out[:, :, 200:241], label[:, :, 5], input_lengths, target_lengths)\n","            # for cm2\n","            loss += critereon(f_out[:, :, 241:282], label[:, :, 6], input_lengths, target_lengths)\n","            # for cm3\n","            loss += critereon(f_out[:, :, 282:323], label[:, :, 7], input_lengths, target_lengths)\n","            # for cm4\n","            loss += critereon(f_out[:, :, 323:364], label[:, :, 8], input_lengths, target_lengths)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            Epoch_loss += loss.item()\n","\n","            del loss\n","            del f_out\n","            del cnn_output\n","            del input_lengths\n","            del k\n","\n","        del image\n","        del label\n","        del num_of_images\n","        del num_of_batches\n","        del target_lengths\n","\n","    Losses.append(Epoch_loss)\n","    print('Epoch: ', Epoch, ' | Loss: ', Epoch_loss, \" | Images: \", number_of_images, 'Time: ', time.time() - start)\n","    del Epoch_loss\n","    del start\n","    del number_of_images\n","    if(Epoch % 100 == 0):\n","        torch.save(network.state_dict(), \"../Saved_Models/Model_rnn\" + str(files) + \".pth\")\n","        torch.save(cnn.state_dict(), \"../Saved_Models/Model_cnn\" + str(files) + \".pth\")\n","        files += 1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","plt.plot(Losses[2:], label = \"Training Loss\", color = 'red')\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.title(\"Loss vs Epochs with BCEWithLogitsLoss\")\n","plt.show()\n","print(Losses[0], Losses[-1])\n","print(Losses)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["torch.save(network.state_dict(), \"/home/ocr/teluguOCR/Saved_Models/Model_2_rnn.pth\")\n","torch.save(cnn.state_dict(), \"/home/ocr/teluguOCR/Saved_Models/Model_2_cnn.pth\")"]},{"cell_type":"markdown","metadata":{},"source":["Testing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# image = torch.load(Images_path + str(1) + '.pt').to(device)\n","# lines = read_file_lines(Labels_path)\n","# label = lines[i-1]\n","# Batch_Image = torch.zeros((image.shape[2] - width + 1)//stride + 1, 1, 30, width).to(device)\n","\n","# for j in range(0, (image.shape[2] - width + 1)//stride):\n","#     Batch_Image[j, 0, :, :] = image[:, :, j*stride:j*stride+width]\n","\n","# cnn_output = cnn(Batch_Image).unsqueeze(1) #batch size x 1 x 50\n","# f_out = torch.zeros(Batch_Image.shape[0], 1, Text_embedding_size).to(device)\n","# f_out[0, 0, :] = network(cnn_output[0], New = True)\n","# for j in range(Batch_Image.shape[0]-1):\n","#     f_out[j+1, 0, :] = network(cnn_output[j]+1, New = False)\n","\n","# #output shape : length x batch x classes\n","# #target shape : batch x length\n","    \n","# print(f_out.shape)\n","# # softmaxing the output\n","# f_out[:, :, :112] = F.softmax(f_out[:, :, :112], dim=2)\n","# f_out[:, :, 112:134] = F.softmax(f_out[:, :, 112:134], dim=2)\n","# f_out[:, :, 134:156] = F.softmax(f_out[:, :, 134:156], dim=2)\n","# f_out[:, :, 156:178] = F.softmax(f_out[:, :, 156:178], dim=2)\n","# f_out[:, :, 178:200] = F.softmax(f_out[:, :, 178:200], dim=2)\n","# f_out[:, :, 200:241] = F.softmax(f_out[:, :, 200:241], dim=2)\n","# f_out[:, :, 241:282] = F.softmax(f_out[:, :, 241:282], dim=2)\n","# f_out[:, :, 282:323] = F.softmax(f_out[:, :, 282:323], dim=2)\n","# f_out[:, :, 323:364] = F.softmax(f_out[:, :, 323:364], dim=2)\n","\n","# print(f_out)\n","\n","\n","# # considering the max probability for each term\n","# max_prob = torch.zeros_like(f_out)\n","# max_prob[:, :, :112] = torch.argmax(f_out[:, :, :112], dim=2).unsqueeze(2)\n","# max_prob[:, :, 112:134] = torch.argmax(f_out[:, :, 112:134], dim=2).unsqueeze(2)\n","# max_prob[:, :, 134:156] = torch.argmax(f_out[:, :, 134:156], dim=2).unsqueeze(2)\n","# max_prob[:, :, 156:178] = torch.argmax(f_out[:, :, 156:178], dim=2).unsqueeze(2)\n","# max_prob[:, :, 178:200] = torch.argmax(f_out[:, :, 178:200], dim=2).unsqueeze(2)\n","# max_prob[:, :, 200:241] = torch.argmax(f_out[:, :, 200:241], dim=2).unsqueeze(2)\n","# max_prob[:, :, 241:282] = torch.argmax(f_out[:, :, 241:282], dim=2).unsqueeze(2)\n","# max_prob[:, :, 282:323] = torch.argmax(f_out[:, :, 282:323], dim=2).unsqueeze(2)\n","# max_prob[:, :, 323:364] = torch.argmax(f_out[:, :, 323:364], dim=2).unsqueeze(2)\n","\n","# print(max_prob.shape)\n","# print(max_prob)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","critereon = nn.CTCLoss(blank=0).cuda() if torch.cuda.is_available() else nn.CTCLoss(blank=0)\n","\n","# assuming 10 classes from 1 to 3 and 0 is the blank class\n","\n","# imput sequence length = 5\n","input = torch.tensor(np.array(([[0, 1, 0, 0], \n","                      [0, 1, 2, 1], \n","                      [0, 1, 1, 2],\n","                      [0, 21, 2000, 1],\n","                      [0, 1, 1, 1]]))).unsqueeze(1).float().cuda()\n","print(input.shape)\n","print(input)\n","input = F.log_softmax(input, dim=2)\n","print(input)\n","\n","# target = torch.tensor(np.array([1, 2])).unsqueeze(0).long().cuda()\n","\n","# print(target.shape)\n","# input_lengths = torch.full(size=(1,), fill_value=5, dtype=torch.long)\n","# target_lengths = torch.full(size=(1,), fill_value=2, dtype=torch.long)\n","\n","# loss = critereon(input, target, input_lengths, target_lengths)\n","# print(loss)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Target are to be padded\n","T = 5      # Input sequence length\n","C = 4      # Number of classes (including blank)\n","N = 1      # Batch size\n","S = 2      # Target sequence length of longest target in batch (padding length)\n","S_min = 1  # Minimum target length, for demonstration purposes\n","\n","# Initialize random batch of input vectors, for *size = (T,N,C)\n","input = torch.randn(T, N, C)\n","print(input)\n","input = F.softmax(input, dim=2)\n","print(input)\n","# Initialize random batch of targets (0 = blank, 1:C = classes)\n","target = torch.randint(low=1, high=C, size=(N, S), dtype=torch.long)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["acchulu = ['అ', 'ఆ', 'ఇ', 'ఈ', 'ఉ', 'ఊ', 'ఋ', 'ౠ', 'ఌ', 'ౡ', 'ఎ', 'ఏ', 'ఐ', 'ఒ', 'ఓ', 'ఔ', 'అం', 'అః']\n","hallulu = ['క', 'ఖ', 'గ', 'ఘ', 'ఙ',\n","           'చ', 'ఛ', 'జ', 'ఝ', 'ఞ',\n","           'ట', 'ఠ', 'డ', 'ఢ', 'ణ',\n","           'త', 'థ', 'ద', 'ధ', 'న',\n","           'ప', 'ఫ', 'బ', 'భ', 'మ',\n","           'య', 'ర', 'ల', 'వ', 'శ', 'ష', 'స', 'హ', 'ళ', 'క్ష', 'ఱ', 'ఴ', 'ౘ', 'ౙ','ౚ']\n","vallulu = ['ా', 'ి', 'ీ', 'ు' , 'ూ', 'ృ', 'ౄ', 'ె', 'ే', 'ై', 'ొ', 'ో', 'ౌ', 'ం', 'ః', 'ఁ', 'ఀ', 'ఄ', 'ౕ', 'ౖ', 'ౢ' ]\n","connector = ['్']\n","numbers = ['౦', '౧', '౨', '౩', '౪', '౫', '౬', '౭', '౮', '౯']\n","splcharacters= [' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')',\n","              '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[',\n","              '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '1','2', '3', '4', '5', '6', '7', '8', '9', '0', 'ఽ']\n","spl = splcharacters + numbers\n","\n","bases = acchulu + hallulu + spl\n","vms = vallulu\n","cms = hallulu\n","\n","characters = bases+vms+cms+connector\n","\n","base_mapping = {}\n","i = 1\n","for x in bases:\n","  base_mapping[x] = i\n","  i+=1\n","\n","vm_mapping = {}\n","i = 2\n","for x in vms:\n","  vm_mapping[x] = i\n","  i+=1\n","\n","cm_mapping = {}\n","i = 2\n","for x in cms:\n","  cm_mapping[x] = i\n","  i+=1\n","\n","# creates a list of ductionaries with each dictionary reporesenting a term\n","def wordsDicts(s):\n","  List = []\n","  for i in range(len(s)):\n","    x = s[i]\n","    prev = ''\n","    if i > 0: prev = s[i-1]\n","    #----------------------------------is it a base term-----------------------\n","    if((x in acchulu or x in hallulu)  and prev != connector[0]):\n","      List.append({})\n","      List[-1]['base'] = x\n","    #----------------------------if it is a consonant modifier-----------------\n","    elif x in hallulu and prev == connector[0]:\n","      if(len(List) == 0):\n","        print(x)\n","      if('cm' not in List[-1]): List[-1]['cm'] = []\n","      List[len(List)-1]['cm'].append(x)\n","\n","      #---------------------------if it is a vowel modifier--------------------\n","    elif x in vallulu:\n","      if(len(List) == 0):\n","        print(x)\n","\n","      if('vm' not in List[-1]): List[-1]['vm'] = []\n","      List[len(List)-1]['vm'].append(x)\n","\n","      #----------------------------it is a spl character-----------------------\n","    elif x in spl:\n","      List.append({})\n","      List[len(List)-1]['base'] = x\n","    else:\n","      continue\n","  return List"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def index_encoding(s):\n","  List = wordsDicts(s)\n","  onehot = []\n","  for i in range(len(List)):\n","    D = List[i]\n","    onehotbase=  [0]\n","    onehotvm1 =  [1]\n","    onehotvm2 =  [1]\n","    onehotvm3 =  [1]\n","    onehotvm4 =  [1]\n","    onehotcm1 =  [1]\n","    onehotcm2 =  [1]\n","    onehotcm3 =  [1]\n","    onehotcm4 =  [1]\n","\n","\n","    onehotbase[0] = base_mapping[D['base']]\n","\n","    it = 1\n","    if('vm' in D):\n","      for j in D['vm']:\n","        if it == 1:\n","          onehotvm1[0] = vm_mapping[j]\n","        elif it == 2:\n","          onehotvm2[0] = vm_mapping[j]\n","        elif it == 3:\n","          onehotvm3[0] = vm_mapping[j]\n","        elif it == 4:\n","          onehotvm4[0] = vm_mapping[j]\n","        it += 1\n","    \n","    it = 1\n","    if('cm' in D):\n","      for j in D['cm']:\n","        if it == 1:\n","          onehotcm1[0] = cm_mapping[j]\n","        elif it == 2:\n","          onehotcm2[0] = cm_mapping[j]\n","        elif it == 3:\n","          onehotcm3[0] = cm_mapping[j]\n","        elif it == 4:\n","          onehotcm4[0] = cm_mapping[j]\n","        it += 1\n","    onehoti = onehotbase + onehotvm1 + onehotvm2 + onehotvm3 + onehotvm4 + onehotcm1 + onehotcm2 + onehotcm3 + onehotcm4 #size 112 + 4*21 + 4*40 = 356\n","    onehot.append(onehoti)\n","  return onehot\n","\n","def index_decoder(List):\n","  x = \"\"\n","  for onehoti in List:\n","    x += bases[onehoti[0]-1]\n","\n","    if onehoti[5] != 1:\n","      x += connector[0]\n","      x += cms[onehoti[5]-2]\n","    if onehoti[6] != 1:\n","      x += connector[0]\n","      x += cms[onehoti[6]-2]\n","    if onehoti[7] != 1:\n","      x += connector[0]\n","      x += cms[onehoti[7]-2]\n","    if onehoti[8] != 1:\n","      x += connector[0]\n","      x += cms[onehoti[8]-2]\n","\n","    if onehoti[1] != 1:\n","      x += vms[onehoti[1]-2]\n","    if onehoti[2] != 1:\n","      x += vms[onehoti[2]-2]\n","    if onehoti[3] != 1:\n","      x += vms[onehoti[3]-2]\n","    if onehoti[4] != 1:\n","      x += vms[onehoti[4]-2]\n","  return x"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["క్కా\n","[[19, 2, 1, 1, 1, 2, 1, 1, 1]]\n","క్కా\n"]}],"source":["s = \"క\" + connector[0] + \"క\" + vms[0]\n","print(s)\n","print(index_encoding(s))\n","print(index_decoder(index_encoding(s)))"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNz6J5cZmFteMEx1b7VjRue","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
