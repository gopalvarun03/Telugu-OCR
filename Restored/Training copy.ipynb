{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3822,"status":"ok","timestamp":1699864430989,"user":{"displayName":"MULUGU VISHWANATH SHARMA","userId":"10455501007410540821"},"user_tz":-330},"id":"41jpf_fwowrj","outputId":"25eab520-4821-4b0a-bb07-be75caefeee3"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import time\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# device = torch.device(\"cpu\")"]},{"cell_type":"markdown","metadata":{},"source":["Structure: "]},{"cell_type":"markdown","metadata":{},"source":["![STRUCTURE](/home/ocr/teluguOCR/Structure.webp) "]},{"cell_type":"markdown","metadata":{},"source":["Hyper parameters of the whole structure"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# out look of the model\n","Number_of_images = 1000\n","Image_size = (32, 32) # (height, width)\n","Image_embedding_size = 5000\n","Text_embedding_size = 358\n","Max_Number_of_Words = 350\n","\n","# Joiner Embedder parameters\n","Joiner_Input_size = Image_embedding_size #5000\n","Joiner_output_size = Text_embedding_size #358\n","\n","# LSTM parameters for the RNN\n","LSTM_Input_size = Joiner_output_size #358\n","LSTM_hidden_size = LSTM_Input_size #358\n","LSTM_num_layers = 1\n","LSTM_output_size = LSTM_hidden_size #358\n","\n","# reverse Embedding parameters\n","Reverse_Input_size = LSTM_output_size #358\n","Reverse_output_size = Text_embedding_size #358\n","\n","drop_prob = 0.2"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["acchulu = ['అ', 'ఆ', 'ఇ', 'ఈ', 'ఉ', 'ఊ', 'ఋ', 'ౠ', 'ఌ', 'ౡ', 'ఎ', 'ఏ', 'ఐ', 'ఒ', 'ఓ', 'ఔ', 'అం', 'అః']\n","hallulu = ['క', 'ఖ', 'గ', 'ఘ', 'ఙ',\n","           'చ', 'ఛ', 'జ', 'ఝ', 'ఞ',\n","           'ట', 'ఠ', 'డ', 'ఢ', 'ణ',\n","           'త', 'థ', 'ద', 'ధ', 'న',\n","           'ప', 'ఫ', 'బ', 'భ', 'మ',\n","           'య', 'ర', 'ల', 'వ', 'శ', 'ష', 'స', 'హ', 'ళ', 'క్ష', 'ఱ', 'ఴ', 'ౘ', 'ౙ','ౚ']\n","vallulu = ['ా', 'ి', 'ీ', 'ు' , 'ూ', 'ృ', 'ౄ', 'ె', 'ే', 'ై', 'ొ', 'ో', 'ౌ', 'ం', 'ః', 'ఁ', 'ఀ', 'ఄ', 'ౕ', 'ౖ', 'ౢ' ]\n","connector = ['్']\n","numbers = ['౦', '౧', '౨', '౩', '౪', '౫', '౬', '౭', '౮', '౯']\n","splcharacters= [' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')',\n","              '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[',\n","              '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '1','2', '3', '4', '5', '6', '7', '8', '9', '0', 'ఽ']\n","spl = splcharacters + numbers\n","\n","bases = acchulu + hallulu + spl\n","vms = vallulu\n","cms = hallulu\n","\n","characters = bases+vms+cms+connector\n","\n","base_mapping = {}\n","i = 1\n","for x in bases:\n","  base_mapping[x] = i\n","  i+=1\n","\n","vm_mapping = {}\n","i = 1\n","for x in vms:\n","  vm_mapping[x] = i\n","  i+=1\n","\n","cm_mapping = {}\n","i = 1\n","for x in cms:\n","  cm_mapping[x] = i\n","  i+=1\n","\n","# creates a list of ductionaries with each dictionary reporesenting a term\n","def wordsDicts(s):\n","  List = []\n","  for i in range(len(s)):\n","    x = s[i]\n","    prev = ''\n","    if i > 0: prev = s[i-1]\n","    #----------------------------------is it a base term-----------------------\n","    if((x in acchulu or x in hallulu)  and prev != connector[0]):\n","      List.append({})\n","      List[-1]['base'] = x\n","    #----------------------------if it is a consonant modifier-----------------\n","    elif x in hallulu and prev == connector[0]:\n","      if(len(List) == 0):\n","        print(x)\n","      if('cm' not in List[-1]): List[-1]['cm'] = []\n","      List[len(List)-1]['cm'].append(x)\n","\n","      #---------------------------if it is a vowel modifier--------------------\n","    elif x in vallulu:\n","      if(len(List) == 0):\n","        print(x)\n","\n","      if('vm' not in List[-1]): List[-1]['vm'] = []\n","      List[len(List)-1]['vm'].append(x)\n","\n","      #----------------------------it is a spl character-----------------------\n","    elif x in spl:\n","      List.append({})\n","      List[len(List)-1]['base'] = x\n","    else:\n","      continue\n","  return List\n","\n","def one_hot_encoder(s):\n","  List = wordsDicts(s)\n","  onehot = []\n","  for i in range(len(List)):\n","    D = List[i]\n","    onehotbase=  [0 for _ in range(len(acchulu) +  len(hallulu) + len(spl))]\n","    onehotvm =  [0 for _ in range(len(vallulu))]\n","    onehotcm =  [0 for _ in range(len(hallulu))]   \n","    onehotbase[base_mapping[D['base']]-1] = 1\n","    if('vm' in D):\n","      for j in D['vm']:\n","        onehotvm[vm_mapping[j]-1] = 1\n","    if('cm' in D):\n","      for j in D['cm']:\n","        onehotcm[cm_mapping[j]-1] = 1\n","    onehoti = [0, 0] + onehotbase + onehotvm + onehotcm # length of 112 + 21 + 40 + 2 = 175\n","    onehot.append(onehoti)\n","  start = [0 for _ in range(175)]\n","  end = [0 for _ in range(175)]\n","  start[0] = 1\n","  end[1] = 1\n","  onehot = [start] + onehot + [end]\n","  encoded = torch.tensor(onehot).float().to(device)\n","  return encoded\n","\n","def One_Hot_Decoder(List):\n","  x = \"\"\n","  for onehoti in List:\n","    onehoti[onehoti >= 0.3] = 1\n","    onehoti[onehoti < 0.3] = 0\n","    for i in range(0, 112):\n","      if onehoti[i+2] == 1:\n","        x += bases[i]\n","    for i in range(133, 173):\n","      if onehoti[i+2] == 1:\n","        x += connector[0] \n","        x += cms[i-133]\n","    for i in range(112,133):\n","      if onehoti[i+2] == 1:\n","        x += vms[i-112]\n","  return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","class EncoderCNN(nn.Module):\n","    def __init__(self) -> None:\n","        super(EncoderCNN, self).__init__()\n","        # input: 30x500\n","        \n","        self.conv1 = nn.Conv2d(1, 10, kernel_size=(3, 101), stride=(2, 1), padding=(0, 0))\n","        # self.conv12 = nn.Conv2d(10, 10, kernel_size=(1, 51), stride=(1, 1), padding=(0, 0))\n","\n","        self.conv2 = nn.Conv2d(10, 20, kernel_size=(3, 51), stride=(1, 1), padding=(0, 0))\n","        # self.conv22 = nn.Conv2d(20, 20, kernel_size=(1, 26), stride=(1, 1), padding=(0, 0))\n","\n","        self.conv3 = nn.Conv2d(20, 30, kernel_size=(3, 51), stride=(1, 1), padding=(0, 0))\n","        # self.conv32 = nn.Conv2d(30, 30, kernel_size=(1, 26), stride=(1, 1), padding=(0, 0))\n","\n","        self.conv4 = nn.Conv2d(30, 40, kernel_size=(3, 101), stride=(2, 1), padding=(0, 0))\n","        # self.conv42 = nn.Conv2d(40, 40, kernel_size=(1, 51), stride=(1, 1), padding=(0, 0))\n","\n","        self.conv5 = nn.Conv2d(40, 50, kernel_size=(3, 101), stride=(2, 1), padding=(0, 0))\n","        # self.conv52 = nn.Conv2d(50, 50, kernel_size=(1, 51), stride=(1, 1), padding=(0, 0))\n","        # output: 50 x 1 x 100\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        # x = F.relu(self.conv12(x))\n","        x = F.relu(self.conv2(x))\n","        # x = F.relu(self.conv22(x))\n","        x = F.relu(self.conv3(x))\n","        # x = F.relu(self.conv32(x))\n","        x = F.relu(self.conv4(x))\n","        # x = F.relu(self.conv42(x))\n","        x = F.relu(self.conv5(x))\n","        # x = F.relu(self.conv52(x))\n","        \n","        # flatten the output\n","        x = x.view(x.size(0), 1, -1)\n","        return x\n","\n","    def configure_optimizers(self):\n","        optimizer = optim.Adam(self.parameters(), lr=0.01)\n","        return optimizer\n","\n","# encoder = EncoderCNN().to(device)\n","# image = torch.randn(20, 1, 30, 500).to(device)\n","# output = encoder(image)\n","# print(output.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# class EncoderCNN(nn.Module):\n","#     def __init__(self):\n","#         super(EncoderCNN, self).__init__()\n","        \n","#         # Convolutional layers\n","#         # input size: (batch_size, 1, 300, 300)\n","#         self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)\n","#         self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n","#         self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n","#         self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n","#         self.conv5 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n","        \n","#         self.relu = nn.LeakyReLU(negative_slope = 0.2)\n","#         # Pooling layers\n","#         self.pool = nn.AvgPool2d(kernel_size=2, stride=2, padding=0)\n","\n","#         # Fully connected layers\n","#         self.fc1 = nn.Linear(512 * 9 * 9, 4096)\n","#         self.fc2 = nn.Linear(4096, 2048)\n","#         self.fc3 = nn.Linear(2048, 1024)\n","#         self.fc4 = nn.Linear(1024, Image_embedding_size)\n","\n","#     def forward(self, x):\n","#         # Input size: (batch_size, 1, 300, 300)\n","\n","#         # Convolutional layers with ReLU activation and pooling\n","#         x = self.pool(self.relu(self.conv1(x))) # (batch_size, 64, 150, 150)\n","#         x = self.pool(self.relu(self.conv2(x))) # (batch_size, 128, 75, 75)\n","#         x = self.pool(self.relu(self.conv3(x))) # (batch_size, 256, 37, 37)\n","#         x = self.pool(self.relu(self.conv4(x))) # (batch_size, 512, 18, 18)\n","#         x = self.pool(self.relu(self.conv5(x))) # (batch_size, 512, 9, 9)\n","\n","#         # Flatten the output before fully connected layers\n","#         x = x.view(-1, 512 * 9 * 9)\n","\n","#         # Fully connected layers\n","#         x = F.relu(self.fc1(x))\n","#         x = F.relu(self.fc2(x))\n","#         x = F.relu(self.fc3(x))\n","#         x = self.fc4(x)\n","\n","#         return x\n","\n","#     def configure_optimizers(self):\n","#         optimizer = optim.Adam(self.parameters(), lr=0.01)\n","#         return optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# import torch.nn as nn\n","# import torch.optim as optim\n","# import torch.nn.functional as F\n","\n","# class EncoderCNN(nn.Module):\n","#     def __init__(self) -> None:\n","#         super(EncoderCNN, self).__init__()\n","#         # input: 32x32\n","#         self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=0)  # 30x30x64\n","#         self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)  # 30x30x64\n","#         self.BatchNorm1 = nn.BatchNorm2d(64, momentum=0.1)\n","#         # self.MaxPool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # 15x15x64\n","#         self.dropout1 = nn.Dropout2d(p=drop_prob)\n","\n","#         self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)  # 15x15x128\n","#         self.conv4 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)  # 15x15x128\n","#         self.BatchNorm2 = nn.BatchNorm2d(128, momentum=0.1)\n","#         self.MaxPool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # 7x7x128\n","#         self.dropout2 = nn.Dropout2d(p=drop_prob)\n","\n","#         self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)  # 7x7x256\n","#         self.conv6 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)  # 7x7x256\n","#         self.BatchNorm3 = nn.BatchNorm2d(256, momentum=0.1)\n","#         self.MaxPool3 = nn.MaxPool2d(kernel_size=2, stride=2) # 3x3x256\n","#         self.dropout3 = nn.Dropout2d(p=drop_prob)\n","\n","#         self.conv7 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)  # 3x3x512\n","#         self.conv8 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)  # 3x3x512\n","#         self.BatchNorm4 = nn.BatchNorm2d(512, momentum=0.1)\n","#         self.MaxPool4 = nn.MaxPool2d(kernel_size=3, stride=3) # 1x1x512\n","#         self.dropout4 = nn.Dropout2d(p=drop_prob)\n","\n","#         # Assuming Image_embedding_size is the output size of the linear layer\n","#         self.Dense = nn.Sequential(\n","#             nn.Linear(512, 1000),\n","#             nn.ReLU(),\n","#             nn.Linear(1000, 500),\n","#             nn.ReLU(),\n","#             nn.Linear(500, Image_embedding_size)\n","#             )\n","\n","#     def forward(self, x):\n","#         # input: 32x32\n","#         x1 = F.relu(self.conv1(x)) # 30x30x64\n","#         x2 = self.conv2(x1) # 30x30x64\n","#         x3 = F.relu(torch.add(x1, self.BatchNorm1(x2))) # skip connection of x1 and x2 (residual connection)\n","#         x = self.MaxPool1(x3) # 15x15x64\n","#         x = self.dropout1(x)\n","\n","#         x1 = F.relu(self.conv3(x)) # 15x15x128\n","#         x2 = self.conv4(x1) # 30x30x64\n","#         x3 = F.relu(torch.add(x1, self.BatchNorm2(x2))) # skip connection of x1 and x2 (residual connection)\n","#         x = self.MaxPool2(x3) # 7x7x128\n","#         x = self.dropout2(x)\n","\n","#         x1 = F.relu(self.conv5(x)) # 7x7x256\n","#         x2 = self.conv6(x1) # 30x30x64\n","#         x3 = F.relu(torch.add(x1, self.BatchNorm3(x2))) # skip connection of x1 and x2 (residual connection)\n","#         x = self.MaxPool3(x3) # 3x3x256\n","#         x = self.dropout3(x)\n","\n","#         x1 = F.relu(self.conv7(x)) # 3x3x512\n","#         x2 = self.conv8(x1) # 30x30x64\n","#         x3 = F.relu(torch.add(x1, self.BatchNorm4(x2))) # skip connection of x1 and x2 (residual connection)\n","#         x = self.MaxPool4(x3) # 1x1x512\n","#         x = self.dropout4(x)\n","\n","#         # Reshape before passing through linear layers\n","#         x = x.view(x.size(0), -1)\n","#         x = self.Dense(x)\n","#         return x\n","\n","#     def configure_optimizers(self):\n","#         optimizer = optim.Adam(self.parameters(), lr=0.01)\n","#         return optimizer\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class LSTM_Net(nn.Module):\n","    def __init__(self) -> None:\n","        super(LSTM_Net, self).__init__()\n","        # embedding layer sizes\n","        self.einput_size = Joiner_Input_size #358\n","        self.eoutput_size = Joiner_output_size #17500\n","        # LSTM parameters\n","        self.embed_size = LSTM_Input_size #17500\n","        self.hidden_size = LSTM_hidden_size #17500\n","        self.num_layers = LSTM_num_layers #1\n","        # reverse embedding layer sizes\n","        self.Rinput_size = Reverse_Input_size #17500\n","        self.Routput_size = Reverse_output_size #358\n","        # dense embedding layers from 358 to 17500\n","        self.embedding1 = nn.Linear(self.einput_size, self.eoutput_size, bias=False)\n","        \n","        # LSTM layer\n","        self.lstm1 = nn.LSTM(input_size = self.embed_size, hidden_size = int(self.embed_size/2) , num_layers = self.num_layers, bidirectional = True, batch_first=True, dropout = drop_prob) #50 to 100\n","        self.lstm2 = nn.LSTM(input_size = self.embed_size, hidden_size = int(self.embed_size/2), num_layers = self.num_layers, bidirectional = True, batch_first=True, dropout = drop_prob) #100 to 200\n","        self.lstm3 = nn.LSTM(input_size = self.embed_size, hidden_size = int(self.embed_size/2) , num_layers = self.num_layers, bidirectional = True, batch_first=True, dropout = drop_prob) #200 to 300\n","\n","        # attention layers for the LSTM\n","        self.attention_Q = nn.Linear(self.Rinput_size, self.Rinput_size)\n","        self.attention_K = nn.Linear(self.Rinput_size, self.Rinput_size)\n","        self.attention_V = nn.Linear(self.Rinput_size, self.Rinput_size)\n","\n","        # dense layers from 17500 to 358\n","        self.Dense1 = nn.Linear(self.Rinput_size, self.Routput_size, bias=False)\n","        \n","        # initialise the weights of the embedding layers\n","        self.relu = nn.ReLU()\n","         \n","    def init_hidden(self, batch_size):\n","        self.hidden1 = (torch.zeros(2*self.num_layers, batch_size, int(self.embed_size/2)).to(device),\n","                torch.zeros(2*self.num_layers, batch_size, int(self.embed_size/2)).to(device))\n","\n","        self.hidden2 = (torch.zeros(2*self.num_layers, batch_size, int(self.embed_size/2)).to(device),\n","                torch.zeros(2*self.num_layers, batch_size, int(self.embed_size/2)).to(device))\n","\n","        self.hidden3 = (torch.zeros(2*self.num_layers, batch_size, int(self.embed_size/2)).to(device),\n","                torch.zeros(2*self.num_layers, batch_size, int(self.embed_size/2)).to(device))\n","        \n","\n","    def forward(self, input, New = False):\n","        if New: # if the input is the image embedding then reset the hidden layers to zeros.\n","            self.init_hidden(input.shape[0])\n","            input = self.embedding1(input) # 358 to 17500 \n","            \n","        # LSTM layers\n","        output1, self.hidden1 = self.lstm1(input, self.hidden1)\n","        output2, self.hidden2 = self.lstm2(output1, self.hidden2)\n","        output3, self.hidden3 = self.lstm3(output2, self.hidden3)\n","\n","        # attention layer\n","        Q = self.attention_Q(output3)\n","        K = self.attention_K(output3)\n","        V = self.attention_V(output3)\n","        attention = torch.bmm(Q, K.transpose(1, 2))\n","        attention = F.softmax(attention, dim=2)\n","        attention = torch.bmm(attention, V)\n","        \n","        # dense layer\n","        attention = F.relu(attention)\n","        attention = self.Dense1(attention)\n","\n","        return attention"]},{"cell_type":"markdown","metadata":{},"source":["Training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["saved_model_losses_min = []\n","saved_model_losses_max = []\n","Losses = []"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cnn = EncoderCNN().to(device)\n","network = LSTM_Net().to(device)\n","\n","# cnn.load_state_dict(torch.load('/home/ocr/teluguOCR/Saved_Models/CNN_latest.pth'))\n","# network.load_state_dict(torch.load('/home/ocr/teluguOCR/Saved_Models/Network_latest.pth'))\n","\n","cnn.train()\n","network.train()\n","\n","params = list(network.parameters()) + list(cnn.parameters())\n","optimizer = optim.Adam(params, lr=1e-5)\n","\n","# gradient clipping\n","clip = 1.0\n","torch.nn.utils.clip_grad_norm_(params, clip, norm_type=2, error_if_nonfinite=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["critereon = nn.MSELoss().cuda() if torch.cuda.is_available() else nn.MSELoss()\n","\n","num_of_epochs = 5000\n","\n","Images_path = \"/home/ocr/teluguOCR/Dataset/Batch_Image_Tensors/Image\"\n","Labels_path = '/home/ocr/teluguOCR/Dataset/Batch_Label_Tensors/Label'\n","\n","def get_data_loader(i):\n","    images = torch.load(Images_path + str(i) + '.pt')\n","    labels = torch.load(Labels_path + str(i) + '.pt')\n","    labels = labels.float()\n","    # labels *= 1e5\n","    return images, labels\n","\n","num = 1\n","Num_of_files = 50\n","\n","for i in range(1, num_of_epochs + 1):\n","        start = time.time()\n","        l_min = 1e18    \n","        l_max = 0\n","        l = 0\n","\n","        # if i%100 == 0:\n","        #     torch.save(network.state_dict(), '/home/ocr/teluguOCR/Saved_Models/Network_latest.pth')\n","        #     torch.save(cnn.state_dict(), '/home/ocr/teluguOCR/Saved_Models/CNN_latest.pth')\n","\n","        # if i == 50:\n","        #     optimizer = optim.Adam(params, lr=1e-6)\n","\n","        num_of_points = 0\n","        batchSize = 500\n","        for j in range(1, Num_of_files + 1):\n","            file_start = time.time()\n","            images, labels = get_data_loader(j)\n","            fl = 0\n","            images = images.to(device)\n","            labels = labels.to(device)\n","            size = images.shape[0]\n","            num_of_points += size\n","            if size > batchSize:\n","                for k in range(0, images.shape[0], batchSize):\n","                    optimizer.zero_grad()\n","                    images_ = images[k:min(k+batchSize, size)]\n","                    labels_ = labels[k:min(k+batchSize, size)]\n","                    features = cnn(images_)\n","                    # features = features.unsqueeze(1)\n","                    outputs = torch.zeros_like(labels_).to(device)\n","                    \n","                    outputs[:, 0, :] = network(features, New = True)[0][0]\n","                    for t in range(labels_.shape[1] - 1):\n","                        outputs[:, t+1, :] = network(labels_[:, t, :].unsqueeze(1) , New = False)[0][0]\n","                    \n","                    # outputs = torch.sigmoid(outputs)\n","                    loss = critereon(outputs, labels_)\n","                    loss.backward()\n","                    optimizer.step()\n","                    fl += loss.item()\n","                    del images_\n","                    del labels_\n","                    del outputs\n","                    del loss  \n","                del images\n","                del labels\n","            else:\n","                optimizer.zero_grad()\n","                features = cnn(images)\n","                # features = features.unsqueeze(1)\n","                outputs = torch.zeros_like(labels).to(device)\n","                \n","                outputs[:, 0, :] = network(features, New = True)[0][0]\n","                for t in range(labels.shape[1] - 1):\n","                    outputs[:, t+1, :] = network(labels[:, t, :].unsqueeze(1) , New = False)[0][0]\n","\n","                # outputs = torch.sigmoid(outputs)\n","                loss = critereon(outputs, labels)\n","                loss.backward()\n","                optimizer.step()\n","                fl += loss.item()\n","                del images\n","                del labels\n","                del outputs\n","                del loss\n","            l_min = min(l_min, fl)\n","            l_max = max(l_max, fl) \n","            l += fl \n","        print(f\"Epoch {i} completed in {format(time.time() - start, '.0f')} seconds with loss ({l_min}, {l_max}), {l}\")\n","        Losses.append(l)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","plt.plot(Losses[2:], label = \"Training Loss\", color = 'red')\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.title(\"Loss vs Epochs with BCEWithLogitsLoss\")\n","plt.show()\n","print(Losses[0], Losses[-1])\n","print(Losses)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# torch.save(network.state_dict(), '/home/ocr/teluguOCR/Saved_Models/Network_1000.pth')\n","# torch.save(cnn.state_dict(), '/home/ocr/teluguOCR/Saved_Models/CNN_1000.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# torch.save(torch.tensor(Losses), '/home/ocr/teluguOCR/Losses/Losses_from_4000_to_5500.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["drthdrhgdsr"]},{"cell_type":"markdown","metadata":{},"source":["Testing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cnn1 = EncoderCNN().to(device)\n","network1 = LSTM_Net().to(device)\n","\n","cnn1.load_state_dict(torch.load('/home/ocr/teluguOCR/Saved_Models/CNN_600.pth'))\n","network1.load_state_dict(torch.load('/home/ocr/teluguOCR/Saved_Models/Network_600.pth'))\n","\n","cnn1.eval()\n","network1.eval()\n","mse = nn.MSELoss()\n","\n","for ind in range(1, 60):\n","    Images = torch.load('/home/ocr/teluguOCR/Dataset/Batch_Image_Tensors/Image' + str(ind) + '.pt').to(device)\n","    Labels = torch.load('/home/ocr/teluguOCR/Dataset/Batch_Label_Tensors/Label' + str(ind) + '.pt').to(device)\n","    loss = 0\n","    print(ind)\n","    if(Labels.shape[0] > 1000):\n","        for k in range(0, Labels.shape[0], 1000):\n","            Image = Images[k:min(k+1000, Labels.shape[0])]\n","            Label = Labels[k:min(k+1000, Labels.shape[0])]\n","            Image = cnn1(Image)\n","            Image = Image.unsqueeze(0)\n","            outputs = network1(Image, Label.shape[1])\n","            outputs = outputs.reshape(outputs.shape[2], outputs.shape[0], outputs.shape[3])\n","            loss += mse(outputs, Label)\n","            del Image\n","            del Label\n","            del outputs\n","    else:\n","        Images = cnn1(Images)\n","        Images = Images.unsqueeze(0)\n","        outputs = network1(Images, Labels.shape[1])\n","        outputs = outputs.reshape(outputs.shape[2], outputs.shape[0], outputs.shape[3])\n","        loss += mse(outputs, Labels)\n","    del Images\n","    del Labels\n","    print(loss)\n","    del loss"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cnn = EncoderCNN().to(device)\n","network = LSTM_Net().to(device)\n","\n","cnn.load_state_dict(torch.load('/home/ocr/teluguOCR/Saved_Models/CNN_latest.pth'))\n","network.load_state_dict(torch.load('/home/ocr/teluguOCR/Saved_Models/Network_latest.pth'))\n","\n","cnn.eval()\n","network.eval()\n","\n","Image = torch.load('/home/ocr/teluguOCR/Dataset/Batch_Image_Tensors/Image' + str(81) + '.pt').to(device)[0]\n","Label = torch.load('/home/ocr/teluguOCR/Dataset/Batch_Label_Tensors/Label' + str(81) + '.pt').to(device)[0]\n","\n","Image = Image.unsqueeze(0)\n","\n","plt.imshow(Image[0][0].cpu().detach().numpy(), cmap = 'gray')\n","plt.show()\n","\n","print(Label.shape)\n","print(\"actual: \", One_Hot_Decoder(Label.cpu().detach().numpy()))\n","\n","features = cnn(Image)\n","# features = features.unsqueeze(0).unsqueeze(0)\n","\n","def roundoff(output):\n","    # print(output.shape)\n","    f = torch.zeros(output.shape)\n","    x = torch.softmax(output[0][0][:114], dim = 0)\n","    # print(x.shape)\n","    y = torch.sigmoid(output[0][0][114:135])\n","    z = torch.sigmoid(output[0][0][135:])\n","\n","    a = torch.argmax(x)\n","    f[0][0][a] = 1\n","    # considering only top 3 values of y\n","    y_arg = torch.argsort(y, descending = True)\n","    y_arg = y_arg[:3]\n","    for i in y_arg:\n","        f[0][0][i+114] = torch.round(y[i])\n","    # considering only top 4 values of z\n","    z_arg = torch.argsort(z, descending = True)\n","    z_arg = z_arg[:4]\n","    for i in z_arg:\n","        f[0][0][i+135] = torch.round(z[i])\n","\n","    return f        \n","\n","\n","\n","F_output = []\n","features = features.unsqueeze(0)\n","output = network(features, New = True).to(device)\n","# print(output.shape)\n","F_output.append(roundoff(decoder(output)).reshape(175).to(device))\n","# while True:\n","for _ in range(100):\n","    output = network(output, New = False).to(device)\n","    # print(output.shape)\n","    F_output.append(roundoff(decoder(output)).reshape(175).to(device))\n","    if(F_output[-1][1] == 1):\n","        break\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fout = torch.stack(F_output).cpu().detach().numpy()\n","print(fout.shape)\n","print(fout[2][:114])\n","s = One_Hot_Decoder(fout)\n","for c in s:\n","    print(c, end = ' + ')\n","print()\n","print(s)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNz6J5cZmFteMEx1b7VjRue","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
