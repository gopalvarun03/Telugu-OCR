{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper - Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out look of the model\n",
    "Number_of_images = 1000\n",
    "Image_size = (32, 32) # (height, width)\n",
    "Image_embedding_size = 50\n",
    "Text_embedding_size = 40\n",
    "Max_Number_of_Words = 350\n",
    "\n",
    "# Joiner Embedder parameters\n",
    "Joiner_Input_size = Text_embedding_size #40\n",
    "Joiner_output_size = Image_embedding_size #50\n",
    "\n",
    "# LSTM parameters for the RNN\n",
    "LSTM_Input_size = Joiner_output_size #50\n",
    "LSTM_hidden_size = LSTM_Input_size #50\n",
    "LSTM_num_layers = 1\n",
    "LSTM_output_size = LSTM_hidden_size #50\n",
    "\n",
    "# reverse Embedding parameters\n",
    "Reverse_Input_size = LSTM_output_size #50\n",
    "Reverse_output_size = Text_embedding_size #40\n",
    "\n",
    "drop_prob = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN - Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        # input: 32x32\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=0)  # 30x30x64\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)  # 30x30x64\n",
    "        self.BatchNorm1 = nn.BatchNorm2d(64, momentum=0.1)\n",
    "        self.MaxPool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # 15x15x64\n",
    "        self.dropout1 = nn.Dropout2d(p=drop_prob)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)  # 15x15x128\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)  # 15x15x128\n",
    "        self.BatchNorm2 = nn.BatchNorm2d(128, momentum=0.1)\n",
    "        self.MaxPool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # 7x7x128\n",
    "        self.dropout2 = nn.Dropout2d(p=drop_prob)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)  # 7x7x256\n",
    "        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)  # 7x7x256\n",
    "        self.BatchNorm3 = nn.BatchNorm2d(256, momentum=0.1)\n",
    "        self.MaxPool3 = nn.MaxPool2d(kernel_size=2, stride=2) # 3x3x256\n",
    "        self.dropout3 = nn.Dropout2d(p=drop_prob)\n",
    "\n",
    "        self.conv7 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)  # 3x3x512\n",
    "        self.conv8 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)  # 3x3x512\n",
    "        self.BatchNorm4 = nn.BatchNorm2d(512, momentum=0.1)\n",
    "        self.MaxPool4 = nn.MaxPool2d(kernel_size=3, stride=3) # 1x1x512\n",
    "        self.dropout4 = nn.Dropout2d(p=drop_prob)\n",
    "\n",
    "        # Assuming Image_embedding_size is the output size of the linear layer\n",
    "        self.Dense = nn.Sequential(\n",
    "            nn.Linear(512, 1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, Image_embedding_size)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input: 32x32\n",
    "        x1 = F.relu(self.conv1(x)) # 30x30x64\n",
    "        x2 = self.conv2(x1) # 30x30x64\n",
    "        x3 = F.relu(torch.add(x1, self.BatchNorm1(x2))) # skip connection of x1 and x2 (residual connection)\n",
    "        x = self.MaxPool1(x3) # 15x15x64\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x1 = F.relu(self.conv3(x)) # 15x15x128\n",
    "        x2 = self.conv4(x1) # 30x30x64\n",
    "        x3 = F.relu(torch.add(x1, self.BatchNorm2(x2))) # skip connection of x1 and x2 (residual connection)\n",
    "        x = self.MaxPool2(x3) # 7x7x128\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x1 = F.relu(self.conv5(x)) # 7x7x256\n",
    "        x2 = self.conv6(x1) # 30x30x64\n",
    "        x3 = F.relu(torch.add(x1, self.BatchNorm3(x2))) # skip connection of x1 and x2 (residual connection)\n",
    "        x = self.MaxPool3(x3) # 3x3x256\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x1 = F.relu(self.conv7(x)) # 3x3x512\n",
    "        x2 = self.conv8(x1) # 30x30x64\n",
    "        x3 = F.relu(torch.add(x1, self.BatchNorm4(x2))) # skip connection of x1 and x2 (residual connection)\n",
    "        x = self.MaxPool4(x3) # 1x1x512\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        # Reshape before passing through linear layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.Dense(x)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.01)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN - Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Net(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(LSTM_Net, self).__init__()\n",
    "        \n",
    "        # embedding layer sizes\n",
    "        self.einput_size = Joiner_Input_size #175\n",
    "        self.eoutput_size = Joiner_output_size #50\n",
    "        \n",
    "        # LSTM parameters\n",
    "        self.embed_size = LSTM_Input_size #50\n",
    "        self.hidden_size = LSTM_hidden_size #50\n",
    "        self.num_layers = LSTM_num_layers #1\n",
    "        \n",
    "        # reverse embedding layer sizes\n",
    "        self.Rinput_size = Reverse_Input_size #50\n",
    "        self.Routput_size = Reverse_output_size #175\n",
    "\n",
    "        # dense embedding layers from 175 to 20\n",
    "        self.embedding1 = nn.Linear(self.einput_size, self.eoutput_size, bias=False)\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm1 = nn.LSTM(input_size = self.embed_size, hidden_size = 50, num_layers = self.num_layers, bidirectional = True, batch_first=True, dropout = drop_prob) #50 to 100\n",
    "        self.lstm2 = nn.LSTM(input_size = 100, hidden_size = 100, num_layers = self.num_layers, bidirectional = True, batch_first=True) #100 to 200\n",
    "        self.lstm3 = nn.LSTM(input_size = 200, hidden_size = 150, num_layers = self.num_layers, bidirectional = True, batch_first=True, dropout = drop_prob) #200 to 300\n",
    "        self.lstm4 = nn.LSTM(input_size = 300, hidden_size = 200, num_layers = self.num_layers, bidirectional = True, batch_first=True) #300 to 400\n",
    "        self.lstm5 = nn.LSTM(input_size = 400, hidden_size = 250, num_layers = self.num_layers, bidirectional = True, batch_first=True, dropout = drop_prob) #400 to 500\n",
    "        self.lstm6 = nn.LSTM(input_size = 500, hidden_size = 250, num_layers = self.num_layers, bidirectional = True, batch_first=True) #500 to 500\n",
    "        self.lstm7 = nn.LSTM(input_size = 500, hidden_size = 250, num_layers = self.num_layers, bidirectional = True, batch_first=True, dropout = drop_prob) #500 to 500\n",
    "        self.lstm8 = nn.LSTM(input_size = 500, hidden_size = 200, num_layers = self.num_layers, bidirectional = True, batch_first=True) #500 to 400\n",
    "        self.lstm9 = nn.LSTM(input_size = 400, hidden_size = 150, num_layers = self.num_layers, bidirectional = True, batch_first=True, dropout = drop_prob) #400 to 300\n",
    "        self.lstm10 = nn.LSTM(input_size = 300, hidden_size = 100, num_layers = self.num_layers, bidirectional = True, batch_first=True) #300 to 200\n",
    "        self.lstm11 = nn.LSTM(input_size = 200, hidden_size = 50, num_layers = self.num_layers, bidirectional = True, batch_first=True, dropout = drop_prob) #200 to 100\n",
    "        self.lstm12 = nn.LSTM(input_size = 100, hidden_size = 25, num_layers = self.num_layers, bidirectional = True, batch_first=True) #100 to 50\n",
    "\n",
    "        # attention layers for the LSTM\n",
    "        self.attention_Q = nn.Linear(self.Rinput_size, self.Rinput_size)\n",
    "        self.attention_K = nn.Linear(self.Rinput_size, self.Rinput_size)\n",
    "        self.attention_V = nn.Linear(self.Rinput_size, self.Rinput_size)\n",
    "\n",
    "        # dense layers from 20 to 175\n",
    "        self.Dense1 = nn.Linear(self.Rinput_size, self.Routput_size, bias=False)\n",
    "\n",
    "        # initialise the weights of the embedding layers\n",
    "        self.relu = nn.ReLU()\n",
    "         \n",
    "    def init_hidden(self, batch_size):\n",
    "        self.hidden1 = (torch.zeros(2*self.num_layers, batch_size, 50).to(device),\n",
    "                torch.zeros(2*self.num_layers, batch_size, 50).to(device))\n",
    "\n",
    "        self.hidden2 = (torch.zeros(2*self.num_layers, batch_size, 100).to(device),\n",
    "                torch.zeros(2*self.num_layers, batch_size, 100).to(device))\n",
    "\n",
    "        self.hidden3 = (torch.zeros(2*self.num_layers, batch_size, 150).to(device),\n",
    "                torch.zeros(2*self.num_layers, batch_size, 150).to(device))\n",
    "\n",
    "        self.hidden4 = (torch.zeros(2*self.num_layers, batch_size, 200).to(device),\n",
    "                torch.zeros(2*self.num_layers, batch_size, 200).to(device))\n",
    "\n",
    "        self.hidden5 = (torch.zeros(2*self.num_layers, batch_size, 250).to(device),\n",
    "                torch.zeros(2*self.num_layers, batch_size, 250).to(device))\n",
    "                \n",
    "        self.hidden6 = (torch.zeros(2*self.num_layers, batch_size, 250).to(device),\n",
    "                torch.zeros(2*self.num_layers, batch_size, 250).to(device))\n",
    "        \n",
    "        self.hidden7 = (torch.zeros(2*self.num_layers, batch_size, 250).to(device),\n",
    "                torch.zeros(2*self.num_layers, batch_size, 250).to(device))\n",
    "                \n",
    "        self.hidden8 = (torch.zeros(2*self.num_layers, batch_size, 200).to(device),\n",
    "                torch.zeros(2*self.num_layers, batch_size, 200).to(device)) \n",
    "                \n",
    "        self.hidden9 = (torch.zeros(2*self.num_layers, batch_size, 150).to(device),\n",
    "                torch.zeros(2*self.num_layers, batch_size, 150).to(device))\n",
    "\n",
    "        self.hidden10 = (torch.zeros(2*self.num_layers, batch_size, 100).to(device),\n",
    "                torch.zeros(2*self.num_layers, batch_size, 100).to(device))\n",
    "        \n",
    "        self.hidden11 = (torch.zeros(2*self.num_layers, batch_size, 50).to(device),\n",
    "                torch.zeros(2*self.num_layers, batch_size, 50).to(device))\n",
    "        \n",
    "        self.hidden12 = (torch.zeros(2*self.num_layers, batch_size, 25).to(device),\n",
    "                torch.zeros(2*self.num_layers, batch_size, 25).to(device))\n",
    "    \n",
    "\n",
    "    def forward(self, input, New = False):\n",
    "        \n",
    "        if New: # if the input is the image embedding then reset the hidden layers to zeros.\n",
    "            self.init_hidden(input.shape[0])\n",
    "        else:\n",
    "            # embedding for input if the onput is not the image embedding.\n",
    "            input = self.embedding1(input) # 40 to 50\n",
    "        \n",
    "        # LSTM layers\n",
    "        output1, self.hidden1 = self.lstm1(input, self.hidden1)\n",
    "        output1 = self.relu(output1)\n",
    "        output2, self.hidden2 = self.lstm2(output1, self.hidden2)\n",
    "        output3, self.hidden3 = self.lstm3(output2, self.hidden3)\n",
    "        output3 = self.relu(output3)\n",
    "        output4, self.hidden4 = self.lstm4(output3, self.hidden4)\n",
    "        output5, self.hidden5 = self.lstm5(output4, self.hidden5)\n",
    "        output5 = self.relu(output5)\n",
    "        output6, self.hidden6 = self.lstm6(output5, self.hidden6)\n",
    "        output7, self.hidden7 = self.lstm7(output6, self.hidden7)\n",
    "        output7 = self.relu(output7)\n",
    "        output8, self.hidden8 = self.lstm8(output7, self.hidden8)\n",
    "        output9, self.hidden9 = self.lstm9(output8, self.hidden9)\n",
    "        output9 = self.relu(output9)\n",
    "        output10, self.hidden10 = self.lstm10(output9, self.hidden10)\n",
    "        output11, self.hidden11 = self.lstm11(output10, self.hidden11)\n",
    "        output11 = self.relu(output11)\n",
    "        output12, self.hidden12 = self.lstm12(output11, self.hidden12)\n",
    "\n",
    "\n",
    "        # attention layer\n",
    "        Q = self.attention_Q(output12)\n",
    "        K = self.attention_K(output12)\n",
    "        V = self.attention_V(output12)\n",
    "        attention = torch.bmm(Q, K.transpose(1, 2))\n",
    "        attention = F.softmax(attention, dim=2)\n",
    "        attention = torch.bmm(attention, V)\n",
    "        \n",
    "        # dense layer\n",
    "        attention = self.Dense1(attention)\n",
    "\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto encoder for word embedding from size of 175 to 40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encode(nn.Module):\n",
    "    def __init__(self, input_shape: int, output_shape: int):\n",
    "        super(Encode, self).__init__()\n",
    "        self.encode1 = nn.Linear(input_shape, 100, bias=False)\n",
    "        self.encode2 = nn.Linear(100, output_shape, bias=False)\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.encode1(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        x = self.encode2(x)\n",
    "        return x\n",
    "\n",
    "class Decode(nn.Module):\n",
    "    def __init__(self, input_shape: int, output_shape: int):\n",
    "        super(Decode, self).__init__()\n",
    "        self.decode1 = nn.Linear(input_shape, 100, bias=False)\n",
    "        self.decode2 = nn.Linear(100, output_shape, bias=False)\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.decode1(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        x = self.decode2(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training - Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_losses = []\n",
    "Losses = []\n",
    "Accuracies = []\n",
    "saved_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = EncoderCNN().to(device)\n",
    "network = LSTM_Net().to(device)\n",
    "\n",
    "cnn.load_state_dict(torch.load('/home/ocr/teluguOCR/Saved_Models/CNN_latest.pth'))\n",
    "network.load_state_dict(torch.load('/home/ocr/teluguOCR/Saved_Models/Network_latest.pth'))\n",
    "\n",
    "encoder = Encode(175, 40).to(device)\n",
    "encoder.load_state_dict(torch.load('/home/ocr/teluguOCR/Models/Encode_no_act40.pth'))\n",
    "encoder.eval()\n",
    "\n",
    "decoder = Decode(40, 175).to(device)\n",
    "decoder.load_state_dict(torch.load('/home/ocr/teluguOCR/Models/Decode_no_act40.pth'))\n",
    "decoder.eval()\n",
    "\n",
    "cnn.train()\n",
    "network.train()\n",
    "\n",
    "params = list(network.parameters()) + list(cnn.parameters())\n",
    "optimizer = optim.Adam(params, lr=5e-6)\n",
    "\n",
    "# gradient clipping\n",
    "clip = 10.0\n",
    "torch.nn.utils.clip_grad_norm_(params, clip, norm_type=2, error_if_nonfinite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critereon = nn.MSELoss().cuda() if torch.cuda.is_available() else nn.MSELoss()\n",
    "\n",
    "num_of_epochs = 4500\n",
    "\n",
    "Images_path = \"/home/ocr/teluguOCR/Dataset/Batch_Image_Tensors/Image\"\n",
    "Labels_path = '/home/ocr/teluguOCR/Dataset/Batch_Label_Tensors/Label'\n",
    "\n",
    "def get_data_loader(i):\n",
    "    images = torch.load(Images_path + str(i) + '.pt')\n",
    "    labels = torch.load(Labels_path + str(i) + '.pt')\n",
    "    labels = labels.float()\n",
    "    # labels *= 1e5\n",
    "    return images, labels\n",
    "\n",
    "num = 1\n",
    "Num_of_files = 50\n",
    "\n",
    "for i in range(1, num_of_epochs + 1):\n",
    "        start = time.time()\n",
    "        l_min = 1e18    \n",
    "        l_max = 0\n",
    "        l = 0\n",
    "\n",
    "        # if i%100 == 0:\n",
    "        #     torch.save(network.state_dict(), '/home/ocr/teluguOCR/Saved_Models/Network_latest.pth')\n",
    "        #     torch.save(cnn.state_dict(), '/home/ocr/teluguOCR/Saved_Models/CNN_latest.pth')\n",
    "\n",
    "        num_of_points = 0\n",
    "        batchSize = 1000\n",
    "        for j in range(1, Num_of_files + 1):\n",
    "            file_start = time.time()\n",
    "            images, labels = get_data_loader(j)\n",
    "            fl = 0\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            size = images.shape[0]\n",
    "            num_of_points += size\n",
    "            if size > batchSize:\n",
    "                for k in range(0, images.shape[0], batchSize):\n",
    "                    optimizer.zero_grad()\n",
    "                    images_ = images[k:min(k+batchSize, size)]\n",
    "                    labels_ = labels[k:min(k+batchSize, size)]\n",
    "                    labels_ = encoder(labels_)\n",
    "                    features = cnn(images_)\n",
    "                    features = features.unsqueeze(1)\n",
    "                    outputs = torch.zeros_like(labels_).to(device)\n",
    "                    \n",
    "                    outputs[:, 0, :] = network(features, New = True)[0][0]\n",
    "                    for t in range(labels_.shape[1] - 1):\n",
    "                        outputs[:, t+1, :] = network(labels_[:, t, :].unsqueeze(1) , New = False)[0][0]\n",
    "                    \n",
    "                    # outputs = torch.sigmoid(outputs)\n",
    "                    loss = critereon(outputs, labels_)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    fl += loss.item()\n",
    "                    del images_\n",
    "                    del labels_\n",
    "                    del outputs\n",
    "                    del loss  \n",
    "                del images\n",
    "                del labels\n",
    "            else:\n",
    "                optimizer.zero_grad()\n",
    "                labels = encoder(labels)\n",
    "                features = cnn(images)\n",
    "                features = features.unsqueeze(1)\n",
    "                outputs = torch.zeros_like(labels).to(device)\n",
    "                \n",
    "                outputs[:, 0, :] = network(features, New = True)[0][0]\n",
    "                for k in range(labels.shape[1] - 1):\n",
    "                    outputs[:, k+1, :] = network(labels[:, k, :].unsqueeze(1) , New = False)[0][0]\n",
    "\n",
    "                # outputs = torch.sigmoid(outputs)\n",
    "                loss = critereon(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                fl += loss.item()\n",
    "                del images\n",
    "                del labels\n",
    "                del outputs\n",
    "                del loss\n",
    "            l_min = min(l_min, fl)\n",
    "            l_max = max(l_max, fl) \n",
    "            l += fl \n",
    "        print(f\"Epoch {i} completed in {format(time.time() - start, '.0f')} seconds with loss ({l_min}, {l_max}), {l}\")\n",
    "        Losses.append(l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teluguOCR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
